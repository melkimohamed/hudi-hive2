<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HiveSyncTool.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-hive</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.hive</a> &gt; <span class="el_source">HiveSyncTool.java</span></div><h1>HiveSyncTool.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.hive;

import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.exception.InvalidTableException;
import org.apache.hudi.hadoop.HoodieParquetInputFormat;
import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
import org.apache.hudi.hive.HoodieHiveClient.PartitionEvent;
import org.apache.hudi.hive.HoodieHiveClient.PartitionEvent.PartitionEventType;
import org.apache.hudi.hive.util.SchemaUtil;

import com.beust.jcommander.JCommander;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.api.Partition;
import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;
import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.parquet.schema.MessageType;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Tool to sync a hoodie HDFS table with a hive metastore table. Either use it as a api
 * HiveSyncTool.syncHoodieTable(HiveSyncConfig) or as a command line java -cp hoodie-hive.jar HiveSyncTool [args]
 * &lt;p&gt;
 * This utility will get the schema from the latest commit and will sync hive table schema Also this will sync the
 * partitions incrementally (all the partitions modified since the last commit)
 */
@SuppressWarnings(&quot;WeakerAccess&quot;)
public class HiveSyncTool {

<span class="nc" id="L55">  private static final Logger LOG = LogManager.getLogger(HiveSyncTool.class);</span>
  public static final String SUFFIX_SNAPSHOT_TABLE = &quot;_rt&quot;;
  public static final String SUFFIX_READ_OPTIMIZED_TABLE = &quot;_ro&quot;;

  private final HiveSyncConfig cfg;
  private final HoodieHiveClient hoodieHiveClient;
  private final String snapshotTableName;
  private final Option&lt;String&gt; roTableTableName;

<span class="nc" id="L64">  public HiveSyncTool(HiveSyncConfig cfg, HiveConf configuration, FileSystem fs) {</span>
<span class="nc" id="L65">    this.hoodieHiveClient = new HoodieHiveClient(cfg, configuration, fs);</span>
<span class="nc" id="L66">    this.cfg = cfg;</span>
<span class="nc bnc" id="L67" title="All 3 branches missed.">    switch (hoodieHiveClient.getTableType()) {</span>
      case COPY_ON_WRITE:
<span class="nc" id="L69">        this.snapshotTableName = cfg.tableName;</span>
<span class="nc" id="L70">        this.roTableTableName = Option.empty();</span>
<span class="nc" id="L71">        break;</span>
      case MERGE_ON_READ:
<span class="nc" id="L73">        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;</span>
<span class="nc bnc" id="L74" title="All 2 branches missed.">        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :</span>
<span class="nc" id="L75">            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);</span>
<span class="nc" id="L76">        break;</span>
      default:
<span class="nc" id="L78">        LOG.error(&quot;Unknown table type &quot; + hoodieHiveClient.getTableType());</span>
<span class="nc" id="L79">        throw new InvalidTableException(hoodieHiveClient.getBasePath());</span>
    }
<span class="nc" id="L81">  }</span>

  public void syncHoodieTable() {
    try {
<span class="nc bnc" id="L85" title="All 3 branches missed.">      switch (hoodieHiveClient.getTableType()) {</span>
        case COPY_ON_WRITE:
<span class="nc" id="L87">          syncHoodieTable(snapshotTableName, false);</span>
<span class="nc" id="L88">          break;</span>
        case MERGE_ON_READ:
          // sync a RO table for MOR
<span class="nc" id="L91">          syncHoodieTable(roTableTableName.get(), false);</span>
          // sync a RT table for MOR
<span class="nc" id="L93">          syncHoodieTable(snapshotTableName, true);</span>
<span class="nc" id="L94">          break;</span>
        default:
<span class="nc" id="L96">          LOG.error(&quot;Unknown table type &quot; + hoodieHiveClient.getTableType());</span>
<span class="nc" id="L97">          throw new InvalidTableException(hoodieHiveClient.getBasePath());</span>
      }
<span class="nc" id="L99">    } catch (RuntimeException re) {</span>
<span class="nc" id="L100">      LOG.error(&quot;Got runtime exception when hive syncing&quot;, re);</span>
    } finally {
<span class="nc" id="L102">      hoodieHiveClient.close();</span>
    }
<span class="nc" id="L104">  }</span>

  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {
<span class="nc" id="L107">    LOG.info(&quot;Trying to sync hoodie table &quot; + tableName + &quot; with base path &quot; + hoodieHiveClient.getBasePath()</span>
<span class="nc" id="L108">        + &quot; of type &quot; + hoodieHiveClient.getTableType());</span>
    // Check if the necessary table exists
<span class="nc" id="L110">    boolean tableExists = hoodieHiveClient.doesTableExist(tableName);</span>
    // Get the parquet schema for this table looking at the latest commit
<span class="nc" id="L112">    MessageType schema = hoodieHiveClient.getDataSchema();</span>
    // Sync schema if needed
<span class="nc" id="L114">    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);</span>

<span class="nc" id="L116">    LOG.info(&quot;Schema sync complete. Syncing partitions for &quot; + tableName);</span>
    // Get the last time we successfully synced partitions
<span class="nc" id="L118">    Option&lt;String&gt; lastCommitTimeSynced = Option.empty();</span>
<span class="nc bnc" id="L119" title="All 2 branches missed.">    if (tableExists) {</span>
<span class="nc" id="L120">      lastCommitTimeSynced = hoodieHiveClient.getLastCommitTimeSynced(tableName);</span>
    }
<span class="nc" id="L122">    LOG.info(&quot;Last commit time synced was found to be &quot; + lastCommitTimeSynced.orElse(&quot;null&quot;));</span>
<span class="nc" id="L123">    List&lt;String&gt; writtenPartitionsSince = hoodieHiveClient.getPartitionsWrittenToSince(lastCommitTimeSynced);</span>
<span class="nc" id="L124">    LOG.info(&quot;Storage partitions scan complete. Found &quot; + writtenPartitionsSince.size());</span>
    // Sync the partitions if needed
<span class="nc" id="L126">    syncPartitions(tableName, writtenPartitionsSince);</span>

<span class="nc" id="L128">    hoodieHiveClient.updateLastCommitTimeSynced(tableName);</span>
<span class="nc" id="L129">    LOG.info(&quot;Sync complete for &quot; + tableName);</span>
<span class="nc" id="L130">  }</span>

  /**
   * Get the latest schema from the last commit and check if its in sync with the hive table schema. If not, evolves the
   * table schema.
   *
   * @param tableExists - does table exist
   * @param schema - extracted schema
   */
  private void syncSchema(String tableName, boolean tableExists, boolean useRealTimeInputFormat, MessageType schema) {
    // Check and sync schema
<span class="nc bnc" id="L141" title="All 2 branches missed.">    if (!tableExists) {</span>
<span class="nc" id="L142">      LOG.info(&quot;Hive table &quot; + tableName + &quot; is not found. Creating it&quot;);</span>
<span class="nc bnc" id="L143" title="All 2 branches missed.">      if (!useRealTimeInputFormat) {</span>
<span class="nc bnc" id="L144" title="All 2 branches missed.">        String inputFormatClassName = cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.HoodieInputFormat.class.getName()</span>
<span class="nc" id="L145">            : HoodieParquetInputFormat.class.getName();</span>
<span class="nc" id="L146">        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),</span>
<span class="nc" id="L147">            ParquetHiveSerDe.class.getName());</span>
<span class="nc" id="L148">      } else {</span>
        // Custom serde will not work with ALTER TABLE REPLACE COLUMNS
        // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive
        // /ql/exec/DDLTask.java#L3488
<span class="nc" id="L152">        String inputFormatClassName =</span>
<span class="nc bnc" id="L153" title="All 2 branches missed.">            cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()</span>
<span class="nc" id="L154">                : HoodieParquetRealtimeInputFormat.class.getName();</span>
<span class="nc" id="L155">        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),</span>
<span class="nc" id="L156">            ParquetHiveSerDe.class.getName());</span>
<span class="nc" id="L157">      }</span>
    } else {
      // Check if the table schema has evolved
<span class="nc" id="L160">      Map&lt;String, String&gt; tableSchema = hoodieHiveClient.getTableSchema(tableName);</span>
<span class="nc" id="L161">      SchemaDifference schemaDiff = SchemaUtil.getSchemaDifference(schema, tableSchema, cfg.partitionFields);</span>
<span class="nc bnc" id="L162" title="All 2 branches missed.">      if (!schemaDiff.isEmpty()) {</span>
<span class="nc" id="L163">        LOG.info(&quot;Schema difference found for &quot; + tableName);</span>
<span class="nc" id="L164">        hoodieHiveClient.updateTableDefinition(tableName, schema);</span>
      } else {
<span class="nc" id="L166">        LOG.info(&quot;No Schema difference for &quot; + tableName);</span>
      }
    }
<span class="nc" id="L169">  }</span>

  /**
   * Syncs the list of storage parititions passed in (checks if the partition is in hive, if not adds it or if the
   * partition path does not match, it updates the partition path).
   */
  private void syncPartitions(String tableName, List&lt;String&gt; writtenPartitionsSince) {
    try {
<span class="nc" id="L177">      List&lt;Partition&gt; hivePartitions = hoodieHiveClient.scanTablePartitions(tableName);</span>
<span class="nc" id="L178">      List&lt;PartitionEvent&gt; partitionEvents =</span>
<span class="nc" id="L179">          hoodieHiveClient.getPartitionEvents(hivePartitions, writtenPartitionsSince);</span>
<span class="nc" id="L180">      List&lt;String&gt; newPartitions = filterPartitions(partitionEvents, PartitionEventType.ADD);</span>
<span class="nc" id="L181">      LOG.info(&quot;New Partitions &quot; + newPartitions);</span>
<span class="nc" id="L182">      hoodieHiveClient.addPartitionsToTable(tableName, newPartitions);</span>
<span class="nc" id="L183">      List&lt;String&gt; updatePartitions = filterPartitions(partitionEvents, PartitionEventType.UPDATE);</span>
<span class="nc" id="L184">      LOG.info(&quot;Changed Partitions &quot; + updatePartitions);</span>
<span class="nc" id="L185">      hoodieHiveClient.updatePartitionsToTable(tableName, updatePartitions);</span>
<span class="nc" id="L186">    } catch (Exception e) {</span>
<span class="nc" id="L187">      throw new HoodieHiveSyncException(&quot;Failed to sync partitions for table &quot; + tableName, e);</span>
<span class="nc" id="L188">    }</span>
<span class="nc" id="L189">  }</span>

  private List&lt;String&gt; filterPartitions(List&lt;PartitionEvent&gt; events, PartitionEventType eventType) {
<span class="nc bnc" id="L192" title="All 2 branches missed.">    return events.stream().filter(s -&gt; s.eventType == eventType).map(s -&gt; s.storagePartition)</span>
<span class="nc" id="L193">        .collect(Collectors.toList());</span>
  }

  public static void main(String[] args) {
    // parse the params
<span class="nc" id="L198">    final HiveSyncConfig cfg = new HiveSyncConfig();</span>
<span class="nc" id="L199">    JCommander cmd = new JCommander(cfg, null, args);</span>
<span class="nc bnc" id="L200" title="All 4 branches missed.">    if (cfg.help || args.length == 0) {</span>
<span class="nc" id="L201">      cmd.usage();</span>
<span class="nc" id="L202">      System.exit(1);</span>
    }
<span class="nc" id="L204">    FileSystem fs = FSUtils.getFs(cfg.basePath, new Configuration());</span>
<span class="nc" id="L205">    HiveConf hiveConf = new HiveConf();</span>
<span class="nc" id="L206">    hiveConf.addResource(fs.getConf());</span>
<span class="nc" id="L207">    new HiveSyncTool(cfg, hiveConf, fs).syncHoodieTable();</span>
<span class="nc" id="L208">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>