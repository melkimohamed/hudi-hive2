<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>RealtimeUnmergedRecordReader.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-hadoop-mr</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.hadoop.realtime</a> &gt; <span class="el_source">RealtimeUnmergedRecordReader.java</span></div><h1>RealtimeUnmergedRecordReader.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.hadoop.realtime;

import org.apache.hudi.common.table.log.HoodieUnMergedLogRecordScanner;
import org.apache.hudi.common.util.DefaultSizeEstimator;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;
import org.apache.hudi.common.util.queue.BoundedInMemoryQueueProducer;
import org.apache.hudi.common.util.queue.FunctionBasedQueueProducer;
import org.apache.hudi.common.util.queue.IteratorBasedQueueProducer;
import org.apache.hudi.hadoop.RecordReaderValueIterator;
import org.apache.hudi.hadoop.SafeParquetRecordReaderWrapper;

import org.apache.avro.generic.GenericRecord;
import org.apache.hadoop.io.ArrayWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RecordReader;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

class RealtimeUnmergedRecordReader extends AbstractRealtimeRecordReader
    implements RecordReader&lt;NullWritable, ArrayWritable&gt; {

  // Log Record unmerged scanner
  private final HoodieUnMergedLogRecordScanner logRecordScanner;

  // Parquet record reader
  private final RecordReader&lt;NullWritable, ArrayWritable&gt; parquetReader;

  // Parquet record iterator wrapper for the above reader
  private final RecordReaderValueIterator&lt;NullWritable, ArrayWritable&gt; parquetRecordsIterator;

  // Executor that runs the above producers in parallel
  private final BoundedInMemoryExecutor&lt;ArrayWritable, ArrayWritable, ?&gt; executor;

  // Iterator for the buffer consumer
  private final Iterator&lt;ArrayWritable&gt; iterator;

  /**
   * Construct a Unmerged record reader that parallely consumes both parquet and log records and buffers for upstream
   * clients to consume.
   *
   * @param split File split
   * @param job Job Configuration
   * @param realReader Parquet Reader
   */
  public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,
      RecordReader&lt;NullWritable, ArrayWritable&gt; realReader) {
<span class="nc" id="L71">    super(split, job);</span>
<span class="nc" id="L72">    this.parquetReader = new SafeParquetRecordReaderWrapper(realReader);</span>
    // Iterator for consuming records from parquet file
<span class="nc" id="L74">    this.parquetRecordsIterator = new RecordReaderValueIterator&lt;&gt;(this.parquetReader);</span>
<span class="nc" id="L75">    this.executor = new BoundedInMemoryExecutor&lt;&gt;(getMaxCompactionMemoryInBytes(), getParallelProducers(),</span>
<span class="nc" id="L76">        Option.empty(), x -&gt; x, new DefaultSizeEstimator&lt;&gt;());</span>
    // Consumer of this record reader
<span class="nc" id="L78">    this.iterator = this.executor.getQueue().iterator();</span>
<span class="nc" id="L79">    this.logRecordScanner = new HoodieUnMergedLogRecordScanner(FSUtils.getFs(split.getPath().toString(), jobConf),</span>
<span class="nc" id="L80">        split.getBasePath(), split.getDeltaLogPaths(), getReaderSchema(), split.getMaxCommitTime(),</span>
<span class="nc" id="L81">        Boolean.parseBoolean(jobConf.get(COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),</span>
<span class="nc" id="L82">        false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -&gt; {</span>
          // convert Hoodie log record to Hadoop AvroWritable and buffer
<span class="nc" id="L84">          GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();</span>
<span class="nc" id="L85">          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getWriterSchema());</span>
<span class="nc" id="L86">          this.executor.getQueue().insertRecord(aWritable);</span>
<span class="nc" id="L87">        });</span>
    // Start reading and buffering
<span class="nc" id="L89">    this.executor.startProducers();</span>
<span class="nc" id="L90">  }</span>

  /**
   * Setup log and parquet reading in parallel. Both write to central buffer.
   */
  private List&lt;BoundedInMemoryQueueProducer&lt;ArrayWritable&gt;&gt; getParallelProducers() {
<span class="nc" id="L96">    List&lt;BoundedInMemoryQueueProducer&lt;ArrayWritable&gt;&gt; producers = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L97">    producers.add(new FunctionBasedQueueProducer&lt;&gt;(buffer -&gt; {</span>
<span class="nc" id="L98">      logRecordScanner.scan();</span>
<span class="nc" id="L99">      return null;</span>
    }));
<span class="nc" id="L101">    producers.add(new IteratorBasedQueueProducer&lt;&gt;(parquetRecordsIterator));</span>
<span class="nc" id="L102">    return producers;</span>
  }

  @Override
  public boolean next(NullWritable key, ArrayWritable value) {
<span class="nc bnc" id="L107" title="All 2 branches missed.">    if (!iterator.hasNext()) {</span>
<span class="nc" id="L108">      return false;</span>
    }
    // Copy from buffer iterator and set to passed writable
<span class="nc" id="L111">    value.set(iterator.next().get());</span>
<span class="nc" id="L112">    return true;</span>
  }

  @Override
  public NullWritable createKey() {
<span class="nc" id="L117">    return parquetReader.createKey();</span>
  }

  @Override
  public ArrayWritable createValue() {
<span class="nc" id="L122">    return parquetReader.createValue();</span>
  }

  @Override
  public long getPos() {
    // TODO: vb - No logical way to represent parallel stream pos in a single long.
    // Should we just return invalid (-1). Where is it used ?
<span class="nc" id="L129">    return 0;</span>
  }

  @Override
  public void close() throws IOException {
<span class="nc" id="L134">    this.parquetRecordsIterator.close();</span>
<span class="nc" id="L135">    this.executor.shutdownNow();</span>
<span class="nc" id="L136">  }</span>

  @Override
  public float getProgress() throws IOException {
<span class="nc" id="L140">    return Math.min(parquetReader.getProgress(), logRecordScanner.getProgress());</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>