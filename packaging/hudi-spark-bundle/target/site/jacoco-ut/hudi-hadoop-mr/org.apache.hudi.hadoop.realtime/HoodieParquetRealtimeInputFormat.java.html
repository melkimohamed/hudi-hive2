<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieParquetRealtimeInputFormat.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-hadoop-mr</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.hadoop.realtime</a> &gt; <span class="el_source">HoodieParquetRealtimeInputFormat.java</span></div><h1>HoodieParquetRealtimeInputFormat.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.hadoop.realtime;

import org.apache.hudi.common.model.FileSlice;
import org.apache.hudi.common.model.HoodieLogFile;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.table.HoodieTableMetaClient;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.table.timeline.HoodieInstant;
import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.exception.HoodieException;
import org.apache.hudi.exception.HoodieIOException;
import org.apache.hudi.hadoop.HoodieParquetInputFormat;
import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;

import com.google.common.base.Preconditions;
import com.google.common.collect.Sets;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
import org.apache.hadoop.io.ArrayWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.function.Function;
import java.util.stream.Collectors;
import java.util.stream.Stream;

/**
 * Input Format, that provides a real-time view of data in a Hoodie table.
 */
@UseFileSplitsFromInputFormat
<span class="nc" id="L66">public class HoodieParquetRealtimeInputFormat extends HoodieParquetInputFormat implements Configurable {</span>

<span class="nc" id="L68">  private static final Logger LOG = LogManager.getLogger(HoodieParquetRealtimeInputFormat.class);</span>

  // These positions have to be deterministic across all tables
  public static final int HOODIE_COMMIT_TIME_COL_POS = 0;
  public static final int HOODIE_RECORD_KEY_COL_POS = 2;
  public static final int HOODIE_PARTITION_PATH_COL_POS = 3;
  public static final String HOODIE_READ_COLUMNS_PROP = &quot;hoodie.read.columns.set&quot;;
  // To make Hive on Spark queries work with RT tables. Our theory is that due to
  // {@link org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher}
  // not handling empty list correctly, the ParquetRecordReaderWrapper ends up adding the same column ids multiple
  // times which ultimately breaks the query.

  @Override
  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {

<span class="nc" id="L83">    Stream&lt;FileSplit&gt; fileSplits = Arrays.stream(super.getSplits(job, numSplits)).map(is -&gt; (FileSplit) is);</span>

    // obtain all unique parent folders for splits
<span class="nc" id="L86">    Map&lt;Path, List&lt;FileSplit&gt;&gt; partitionsToParquetSplits =</span>
<span class="nc" id="L87">        fileSplits.collect(Collectors.groupingBy(split -&gt; split.getPath().getParent()));</span>
    // TODO(vc): Should we handle also non-hoodie splits here?
<span class="nc" id="L89">    Map&lt;String, HoodieTableMetaClient&gt; metaClientMap = new HashMap&lt;&gt;();</span>
<span class="nc" id="L90">    Map&lt;Path, HoodieTableMetaClient&gt; partitionsToMetaClient =</span>
<span class="nc" id="L91">        partitionsToParquetSplits.keySet().stream().collect(Collectors.toMap(Function.identity(), p -&gt; {</span>
          // find if we have a metaclient already for this partition.
<span class="nc" id="L93">          Option&lt;String&gt; matchingBasePath = Option.fromJavaOptional(</span>
<span class="nc" id="L94">              metaClientMap.keySet().stream().filter(basePath -&gt; p.toString().startsWith(basePath)).findFirst());</span>
<span class="nc bnc" id="L95" title="All 2 branches missed.">          if (matchingBasePath.isPresent()) {</span>
<span class="nc" id="L96">            return metaClientMap.get(matchingBasePath.get());</span>
          }

          try {
<span class="nc" id="L100">            HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);</span>
<span class="nc" id="L101">            metaClientMap.put(metaClient.getBasePath(), metaClient);</span>
<span class="nc" id="L102">            return metaClient;</span>
<span class="nc" id="L103">          } catch (IOException e) {</span>
<span class="nc" id="L104">            throw new HoodieIOException(&quot;Error creating hoodie meta client against : &quot; + p, e);</span>
          }
        }));

    // for all unique split parents, obtain all delta files based on delta commit timeline,
    // grouped on file id
<span class="nc" id="L110">    List&lt;HoodieRealtimeFileSplit&gt; rtSplits = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L111">    partitionsToParquetSplits.keySet().forEach(partitionPath -&gt; {</span>
      // for each partition path obtain the data &amp; log file groupings, then map back to inputsplits
<span class="nc" id="L113">      HoodieTableMetaClient metaClient = partitionsToMetaClient.get(partitionPath);</span>
<span class="nc" id="L114">      HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline());</span>
<span class="nc" id="L115">      String relPartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), partitionPath);</span>

      try {
        // Both commit and delta-commits are included - pick the latest completed one
<span class="nc" id="L119">        Option&lt;HoodieInstant&gt; latestCompletedInstant =</span>
<span class="nc" id="L120">            metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();</span>

<span class="nc" id="L122">        Stream&lt;FileSlice&gt; latestFileSlices = latestCompletedInstant</span>
<span class="nc" id="L123">            .map(instant -&gt; fsView.getLatestMergedFileSlicesBeforeOrOn(relPartitionPath, instant.getTimestamp()))</span>
<span class="nc" id="L124">            .orElse(Stream.empty());</span>

        // subgroup splits again by file id &amp; match with log files.
<span class="nc" id="L127">        Map&lt;String, List&lt;FileSplit&gt;&gt; groupedInputSplits = partitionsToParquetSplits.get(partitionPath).stream()</span>
<span class="nc" id="L128">            .collect(Collectors.groupingBy(split -&gt; FSUtils.getFileId(split.getPath().getName())));</span>
<span class="nc" id="L129">        latestFileSlices.forEach(fileSlice -&gt; {</span>
<span class="nc" id="L130">          List&lt;FileSplit&gt; dataFileSplits = groupedInputSplits.get(fileSlice.getFileId());</span>
<span class="nc" id="L131">          dataFileSplits.forEach(split -&gt; {</span>
            try {
<span class="nc" id="L133">              List&lt;String&gt; logFilePaths = fileSlice.getLogFiles().sorted(HoodieLogFile.getLogFileComparator())</span>
<span class="nc" id="L134">                  .map(logFile -&gt; logFile.getPath().toString()).collect(Collectors.toList());</span>
              // Get the maxCommit from the last delta or compaction or commit - when
              // bootstrapped from COW table
<span class="nc" id="L137">              String maxCommitTime = metaClient</span>
<span class="nc" id="L138">                  .getActiveTimeline().getTimelineOfActions(Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,</span>
                      HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))
<span class="nc" id="L140">                  .filterCompletedInstants().lastInstant().get().getTimestamp();</span>
<span class="nc" id="L141">              rtSplits.add(new HoodieRealtimeFileSplit(split, metaClient.getBasePath(), logFilePaths, maxCommitTime));</span>
<span class="nc" id="L142">            } catch (IOException e) {</span>
<span class="nc" id="L143">              throw new HoodieIOException(&quot;Error creating hoodie real time split &quot;, e);</span>
<span class="nc" id="L144">            }</span>
<span class="nc" id="L145">          });</span>
<span class="nc" id="L146">        });</span>
<span class="nc" id="L147">      } catch (Exception e) {</span>
<span class="nc" id="L148">        throw new HoodieException(&quot;Error obtaining data file/log file grouping: &quot; + partitionPath, e);</span>
<span class="nc" id="L149">      }</span>
<span class="nc" id="L150">    });</span>
<span class="nc" id="L151">    LOG.info(&quot;Returning a total splits of &quot; + rtSplits.size());</span>
<span class="nc" id="L152">    return rtSplits.toArray(new InputSplit[0]);</span>
  }

  @Override
  public FileStatus[] listStatus(JobConf job) throws IOException {
    // Call the HoodieInputFormat::listStatus to obtain all latest parquet files, based on commit
    // timeline.
<span class="nc" id="L159">    return super.listStatus(job);</span>
  }

  /**
   * Add a field to the existing fields projected.
   */
  private static Configuration addProjectionField(Configuration conf, String fieldName, int fieldIndex) {
<span class="nc" id="L166">    String readColNames = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, &quot;&quot;);</span>
<span class="nc" id="L167">    String readColIds = conf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, &quot;&quot;);</span>

<span class="nc" id="L169">    String readColNamesPrefix = readColNames + &quot;,&quot;;</span>
<span class="nc bnc" id="L170" title="All 4 branches missed.">    if (readColNames == null || readColNames.isEmpty()) {</span>
<span class="nc" id="L171">      readColNamesPrefix = &quot;&quot;;</span>
    }
<span class="nc" id="L173">    String readColIdsPrefix = readColIds + &quot;,&quot;;</span>
<span class="nc bnc" id="L174" title="All 4 branches missed.">    if (readColIds == null || readColIds.isEmpty()) {</span>
<span class="nc" id="L175">      readColIdsPrefix = &quot;&quot;;</span>
    }

<span class="nc bnc" id="L178" title="All 2 branches missed.">    if (!readColNames.contains(fieldName)) {</span>
      // If not already in the list - then add it
<span class="nc" id="L180">      conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, readColNamesPrefix + fieldName);</span>
<span class="nc" id="L181">      conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, readColIdsPrefix + fieldIndex);</span>
<span class="nc bnc" id="L182" title="All 2 branches missed.">      if (LOG.isDebugEnabled()) {</span>
<span class="nc" id="L183">        LOG.debug(String.format(&quot;Adding extra column &quot; + fieldName + &quot;, to enable log merging cols (%s) ids (%s) &quot;,</span>
<span class="nc" id="L184">            conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR),</span>
<span class="nc" id="L185">            conf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR)));</span>
      }
    }
<span class="nc" id="L188">    return conf;</span>
  }

  private static void addRequiredProjectionFields(Configuration configuration) {
    // Need this to do merge records in HoodieRealtimeRecordReader
<span class="nc" id="L193">    addProjectionField(configuration, HoodieRecord.RECORD_KEY_METADATA_FIELD, HOODIE_RECORD_KEY_COL_POS);</span>
<span class="nc" id="L194">    addProjectionField(configuration, HoodieRecord.COMMIT_TIME_METADATA_FIELD, HOODIE_COMMIT_TIME_COL_POS);</span>
<span class="nc" id="L195">    addProjectionField(configuration, HoodieRecord.PARTITION_PATH_METADATA_FIELD, HOODIE_PARTITION_PATH_COL_POS);</span>
<span class="nc" id="L196">  }</span>

  /**
   * Hive will append read columns' ids to old columns' ids during getRecordReader. In some cases, e.g. SELECT COUNT(*),
   * the read columns' id is an empty string and Hive will combine it with Hoodie required projection ids and becomes
   * e.g. &quot;,2,0,3&quot; and will cause an error. Actually this method is a temporary solution because the real bug is from
   * Hive. Hive has fixed this bug after 3.0.0, but the version before that would still face this problem. (HIVE-22438)
   */
  private static void cleanProjectionColumnIds(Configuration conf) {
<span class="nc" id="L205">    String columnIds = conf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR);</span>
<span class="nc bnc" id="L206" title="All 4 branches missed.">    if (!columnIds.isEmpty() &amp;&amp; columnIds.charAt(0) == ',') {</span>
<span class="nc" id="L207">      conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, columnIds.substring(1));</span>
<span class="nc bnc" id="L208" title="All 2 branches missed.">      if (LOG.isDebugEnabled()) {</span>
<span class="nc" id="L209">        LOG.debug(&quot;The projection Ids: {&quot; + columnIds + &quot;} start with ','. First comma is removed&quot;);</span>
      }
    }
<span class="nc" id="L212">  }</span>

  @Override
  public RecordReader&lt;NullWritable, ArrayWritable&gt; getRecordReader(final InputSplit split, final JobConf jobConf,
      final Reporter reporter) throws IOException {
    // Hive on Spark invokes multiple getRecordReaders from different threads in the same spark task (and hence the
    // same JVM) unlike Hive on MR. Due to this, accesses to JobConf, which is shared across all threads, is at the
    // risk of experiencing race conditions. Hence, we synchronize on the JobConf object here. There is negligible
    // latency incurred here due to the synchronization since get record reader is called once per spilt before the
    // actual heavy lifting of reading the parquet files happen.
<span class="nc bnc" id="L222" title="All 2 branches missed.">    if (jobConf.get(HOODIE_READ_COLUMNS_PROP) == null) {</span>
<span class="nc" id="L223">      synchronized (jobConf) {</span>
<span class="nc" id="L224">        LOG.info(</span>
<span class="nc" id="L225">            &quot;Before adding Hoodie columns, Projections :&quot; + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)</span>
<span class="nc" id="L226">                + &quot;, Ids :&quot; + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));</span>
<span class="nc bnc" id="L227" title="All 2 branches missed.">        if (jobConf.get(HOODIE_READ_COLUMNS_PROP) == null) {</span>
          // Hive (across all versions) fails for queries like select count(`_hoodie_commit_time`) from table;
          // In this case, the projection fields gets removed. Looking at HiveInputFormat implementation, in some cases
          // hoodie additional projection columns are reset after calling setConf and only natural projections
          // (one found in select queries) are set. things would break because of this.
          // For e:g _hoodie_record_key would be missing and merge step would throw exceptions.
          // TO fix this, hoodie columns are appended late at the time record-reader gets built instead of construction
          // time.
<span class="nc" id="L235">          cleanProjectionColumnIds(jobConf);</span>
<span class="nc" id="L236">          addRequiredProjectionFields(jobConf);</span>

<span class="nc" id="L238">          this.conf = jobConf;</span>
<span class="nc" id="L239">          this.conf.set(HOODIE_READ_COLUMNS_PROP, &quot;true&quot;);</span>
        }
<span class="nc" id="L241">      }</span>
    }

<span class="nc" id="L244">    LOG.info(&quot;Creating record reader with readCols :&quot; + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)</span>
<span class="nc" id="L245">        + &quot;, Ids :&quot; + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));</span>
    // sanity check
<span class="nc" id="L247">    Preconditions.checkArgument(split instanceof HoodieRealtimeFileSplit,</span>
        &quot;HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with &quot; + split);

<span class="nc" id="L250">    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,</span>
<span class="nc" id="L251">        super.getRecordReader(split, jobConf, reporter));</span>
  }

  @Override
  public Configuration getConf() {
<span class="nc" id="L256">    return conf;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>