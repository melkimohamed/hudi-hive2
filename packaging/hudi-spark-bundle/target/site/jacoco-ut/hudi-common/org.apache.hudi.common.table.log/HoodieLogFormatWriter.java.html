<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieLogFormatWriter.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-common</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.common.table.log</a> &gt; <span class="el_source">HoodieLogFormatWriter.java</span></div><h1>HoodieLogFormatWriter.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.common.table.log;

import org.apache.hudi.common.model.HoodieLogFile;
import org.apache.hudi.common.storage.StorageSchemes;
import org.apache.hudi.common.table.log.HoodieLogFormat.Writer;
import org.apache.hudi.common.table.log.HoodieLogFormat.WriterBuilder;
import org.apache.hudi.common.table.log.block.HoodieLogBlock;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.exception.HoodieException;
import org.apache.hudi.exception.HoodieIOException;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.DistributedFileSystem;
import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;
import org.apache.hadoop.hdfs.protocol.RecoveryInProgressException;
import org.apache.hadoop.ipc.RemoteException;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;

import java.io.IOException;

/**
 * HoodieLogFormatWriter can be used to append blocks to a log file Use HoodieLogFormat.WriterBuilder to construct.
 */
public class HoodieLogFormatWriter implements HoodieLogFormat.Writer {

<span class="nc" id="L47">  private static final Logger LOG = LogManager.getLogger(HoodieLogFormatWriter.class);</span>

  private HoodieLogFile logFile;
  private final FileSystem fs;
  private final long sizeThreshold;
  private final Integer bufferSize;
  private final Short replication;
  private final String logWriteToken;
  private final String rolloverLogWriteToken;
  private FSDataOutputStream output;
  private static final String APPEND_UNAVAILABLE_EXCEPTION_MESSAGE = &quot;not sufficiently replicated yet&quot;;

  /**
   * @param fs
   * @param logFile
   * @param bufferSize
   * @param replication
   * @param sizeThreshold
   */
  HoodieLogFormatWriter(FileSystem fs, HoodieLogFile logFile, Integer bufferSize, Short replication, Long sizeThreshold,
<span class="nc" id="L67">      String logWriteToken, String rolloverLogWriteToken) throws IOException, InterruptedException {</span>
<span class="nc" id="L68">    this.fs = fs;</span>
<span class="nc" id="L69">    this.logFile = logFile;</span>
<span class="nc" id="L70">    this.sizeThreshold = sizeThreshold;</span>
<span class="nc" id="L71">    this.bufferSize = bufferSize;</span>
<span class="nc" id="L72">    this.replication = replication;</span>
<span class="nc" id="L73">    this.logWriteToken = logWriteToken;</span>
<span class="nc" id="L74">    this.rolloverLogWriteToken = rolloverLogWriteToken;</span>
<span class="nc" id="L75">    addShutDownHook();</span>
<span class="nc" id="L76">    Path path = logFile.getPath();</span>
<span class="nc bnc" id="L77" title="All 2 branches missed.">    if (fs.exists(path)) {</span>
<span class="nc" id="L78">      boolean isAppendSupported = StorageSchemes.isAppendSupported(fs.getScheme());</span>
<span class="nc bnc" id="L79" title="All 2 branches missed.">      if (isAppendSupported) {</span>
<span class="nc" id="L80">        LOG.info(logFile + &quot; exists. Appending to existing file&quot;);</span>
        try {
<span class="nc" id="L82">          this.output = fs.append(path, bufferSize);</span>
<span class="nc" id="L83">        } catch (RemoteException e) {</span>
<span class="nc" id="L84">          LOG.warn(&quot;Remote Exception, attempting to handle or recover lease&quot;, e);</span>
<span class="nc" id="L85">          handleAppendExceptionOrRecoverLease(path, e);</span>
<span class="nc" id="L86">        } catch (IOException ioe) {</span>
<span class="nc bnc" id="L87" title="All 2 branches missed.">          if (ioe.getMessage().toLowerCase().contains(&quot;not supported&quot;)) {</span>
            // may still happen if scheme is viewfs.
<span class="nc" id="L89">            isAppendSupported = false;</span>
          } else {
            /*
             * Before throwing an exception, close the outputstream,
             * to ensure that the lease on the log file is released.
             */
<span class="nc" id="L95">            close();</span>
<span class="nc" id="L96">            throw ioe;</span>
          }
<span class="nc" id="L98">        }</span>
      }
<span class="nc bnc" id="L100" title="All 2 branches missed.">      if (!isAppendSupported) {</span>
<span class="nc" id="L101">        this.logFile = logFile.rollOver(fs, rolloverLogWriteToken);</span>
<span class="nc" id="L102">        LOG.info(&quot;Append not supported.. Rolling over to &quot; + logFile);</span>
<span class="nc" id="L103">        createNewFile();</span>
      }
<span class="nc" id="L105">    } else {</span>
<span class="nc" id="L106">      LOG.info(logFile + &quot; does not exist. Create a new file&quot;);</span>
      // Block size does not matter as we will always manually autoflush
<span class="nc" id="L108">      createNewFile();</span>
    }
<span class="nc" id="L110">  }</span>

  public FileSystem getFs() {
<span class="nc" id="L113">    return fs;</span>
  }

  @Override
  public HoodieLogFile getLogFile() {
<span class="nc" id="L118">    return logFile;</span>
  }

  public long getSizeThreshold() {
<span class="nc" id="L122">    return sizeThreshold;</span>
  }

  @Override
  public Writer appendBlock(HoodieLogBlock block) throws IOException, InterruptedException {

    // Find current version
<span class="nc" id="L129">    HoodieLogFormat.LogFormatVersion currentLogFormatVersion =</span>
        new HoodieLogFormatVersion(HoodieLogFormat.CURRENT_VERSION);
<span class="nc" id="L131">    long currentSize = this.output.size();</span>

    // 1. Write the magic header for the start of the block
<span class="nc" id="L134">    this.output.write(HoodieLogFormat.MAGIC);</span>

    // bytes for header
<span class="nc" id="L137">    byte[] headerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockHeader());</span>
    // content bytes
<span class="nc" id="L139">    byte[] content = block.getContentBytes();</span>
    // bytes for footer
<span class="nc" id="L141">    byte[] footerBytes = HoodieLogBlock.getLogMetadataBytes(block.getLogBlockFooter());</span>

    // 2. Write the total size of the block (excluding Magic)
<span class="nc" id="L144">    this.output.writeLong(getLogBlockLength(content.length, headerBytes.length, footerBytes.length));</span>

    // 3. Write the version of this log block
<span class="nc" id="L147">    this.output.writeInt(currentLogFormatVersion.getVersion());</span>
    // 4. Write the block type
<span class="nc" id="L149">    this.output.writeInt(block.getBlockType().ordinal());</span>

    // 5. Write the headers for the log block
<span class="nc" id="L152">    this.output.write(headerBytes);</span>
    // 6. Write the size of the content block
<span class="nc" id="L154">    this.output.writeLong(content.length);</span>
    // 7. Write the contents of the data block
<span class="nc" id="L156">    this.output.write(content);</span>
    // 8. Write the footers for the log block
<span class="nc" id="L158">    this.output.write(footerBytes);</span>
    // 9. Write the total size of the log block (including magic) which is everything written
    // until now (for reverse pointer)
    // Update: this information is now used in determining if a block is corrupt by comparing to the
    //   block size in header. This change assumes that the block size will be the last data written
    //   to a block. Read will break if any data is written past this point for a block.
<span class="nc" id="L164">    this.output.writeLong(this.output.size() - currentSize);</span>
    // Flush every block to disk
<span class="nc" id="L166">    flush();</span>

    // roll over if size is past the threshold
<span class="nc" id="L169">    return rolloverIfNeeded();</span>
  }

  /**
   * This method returns the total LogBlock Length which is the sum of 1. Number of bytes to write version 2. Number of
   * bytes to write ordinal 3. Length of the headers 4. Number of bytes used to write content length 5. Length of the
   * content 6. Length of the footers 7. Number of bytes to write totalLogBlockLength
   */
  private int getLogBlockLength(int contentLength, int headerLength, int footerLength) {
<span class="nc" id="L178">    return Integer.BYTES + // Number of bytes to write version</span>
        Integer.BYTES + // Number of bytes to write ordinal
        headerLength + // Length of the headers
        Long.BYTES + // Number of bytes used to write content length
        contentLength + // Length of the content
        footerLength + // Length of the footers
        Long.BYTES; // bytes to write totalLogBlockLength at end of block (for reverse ptr)
  }

  private Writer rolloverIfNeeded() throws IOException, InterruptedException {
    // Roll over if the size is past the threshold
<span class="nc bnc" id="L189" title="All 2 branches missed.">    if (getCurrentSize() &gt; sizeThreshold) {</span>
      // TODO - make an end marker which seals the old log file (no more appends possible to that
      // file).
<span class="nc" id="L192">      LOG.info(&quot;CurrentSize &quot; + getCurrentSize() + &quot; has reached threshold &quot; + sizeThreshold</span>
          + &quot;. Rolling over to the next version&quot;);
<span class="nc" id="L194">      HoodieLogFile newLogFile = logFile.rollOver(fs, rolloverLogWriteToken);</span>
      // close this writer and return the new writer
<span class="nc" id="L196">      close();</span>
<span class="nc" id="L197">      return new HoodieLogFormatWriter(fs, newLogFile, bufferSize, replication, sizeThreshold, logWriteToken,</span>
          rolloverLogWriteToken);
    }
<span class="nc" id="L200">    return this;</span>
  }

  private void createNewFile() throws IOException {
<span class="nc" id="L204">    this.output =</span>
<span class="nc" id="L205">        fs.create(this.logFile.getPath(), false, bufferSize, replication, WriterBuilder.DEFAULT_SIZE_THRESHOLD, null);</span>
<span class="nc" id="L206">  }</span>

  @Override
  public void close() throws IOException {
<span class="nc" id="L210">    flush();</span>
<span class="nc" id="L211">    output.close();</span>
<span class="nc" id="L212">    output = null;</span>
<span class="nc" id="L213">  }</span>

  private void flush() throws IOException {
<span class="nc bnc" id="L216" title="All 2 branches missed.">    if (output == null) {</span>
<span class="nc" id="L217">      return; // Presume closed</span>
    }
<span class="nc" id="L219">    output.flush();</span>
    // NOTE : the following API call makes sure that the data is flushed to disk on DataNodes (akin to POSIX fsync())
    // See more details here : https://issues.apache.org/jira/browse/HDFS-744
<span class="nc" id="L222">    output.hsync();</span>
<span class="nc" id="L223">  }</span>

  @Override
  public long getCurrentSize() throws IOException {
<span class="nc bnc" id="L227" title="All 2 branches missed.">    if (output == null) {</span>
<span class="nc" id="L228">      throw new IllegalStateException(&quot;Cannot get current size as the underlying stream has been closed already&quot;);</span>
    }
<span class="nc" id="L230">    return output.getPos();</span>
  }

  /**
   * Close the output stream when the JVM exits.
   */
  private void addShutDownHook() {
<span class="nc" id="L237">    Runtime.getRuntime().addShutdownHook(new Thread() {</span>
      public void run() {
        try {
<span class="nc bnc" id="L240" title="All 2 branches missed.">          if (output != null) {</span>
<span class="nc" id="L241">            close();</span>
          }
<span class="nc" id="L243">        } catch (Exception e) {</span>
<span class="nc" id="L244">          LOG.warn(&quot;unable to close output stream for log file &quot; + logFile, e);</span>
          // fail silently for any sort of exception
<span class="nc" id="L246">        }</span>
<span class="nc" id="L247">      }</span>
    });
<span class="nc" id="L249">  }</span>

  private void handleAppendExceptionOrRecoverLease(Path path, RemoteException e)
      throws IOException, InterruptedException {
<span class="nc bnc" id="L253" title="All 2 branches missed.">    if (e.getMessage().contains(APPEND_UNAVAILABLE_EXCEPTION_MESSAGE)) {</span>
      // This issue happens when all replicas for a file are down and/or being decommissioned.
      // The fs.append() API could append to the last block for a file. If the last block is full, a new block is
      // appended to. In a scenario when a lot of DN's are decommissioned, it can happen that DN's holding all
      // replicas for a block/file are decommissioned together. During this process, all these blocks will start to
      // get replicated to other active DataNodes but this process might take time (can be of the order of few
      // hours). During this time, if a fs.append() API is invoked for a file whose last block is eligible to be
      // appended to, then the NN will throw an exception saying that it couldn't find any active replica with the
      // last block. Find more information here : https://issues.apache.org/jira/browse/HDFS-6325
<span class="nc" id="L262">      LOG.warn(&quot;Failed to open an append stream to the log file. Opening a new log file..&quot;, e);</span>
      // Rollover the current log file (since cannot get a stream handle) and create new one
<span class="nc" id="L264">      this.logFile = logFile.rollOver(fs, rolloverLogWriteToken);</span>
<span class="nc" id="L265">      createNewFile();</span>
<span class="nc bnc" id="L266" title="All 2 branches missed.">    } else if (e.getClassName().contentEquals(AlreadyBeingCreatedException.class.getName())) {</span>
<span class="nc" id="L267">      LOG.warn(&quot;Another task executor writing to the same log file(&quot; + logFile + &quot;. Rolling over&quot;);</span>
      // Rollover the current log file (since cannot get a stream handle) and create new one
<span class="nc" id="L269">      this.logFile = logFile.rollOver(fs, rolloverLogWriteToken);</span>
<span class="nc" id="L270">      createNewFile();</span>
<span class="nc bnc" id="L271" title="All 4 branches missed.">    } else if (e.getClassName().contentEquals(RecoveryInProgressException.class.getName())</span>
        &amp;&amp; (fs instanceof DistributedFileSystem)) {
      // this happens when either another task executor writing to this file died or
      // data node is going down. Note that we can only try to recover lease for a DistributedFileSystem.
      // ViewFileSystem unfortunately does not support this operation
<span class="nc" id="L276">      LOG.warn(&quot;Trying to recover log on path &quot; + path);</span>
<span class="nc bnc" id="L277" title="All 2 branches missed.">      if (FSUtils.recoverDFSFileLease((DistributedFileSystem) fs, path)) {</span>
<span class="nc" id="L278">        LOG.warn(&quot;Recovered lease on path &quot; + path);</span>
        // try again
<span class="nc" id="L280">        this.output = fs.append(path, bufferSize);</span>
      } else {
<span class="nc" id="L282">        LOG.warn(&quot;Failed to recover lease on path &quot; + path);</span>
<span class="nc" id="L283">        throw new HoodieException(e);</span>
      }
    } else {
      // When fs.append() has failed and an exception is thrown, by closing the output stream
      // we shall force hdfs to release the lease on the log file. When Spark retries this task (with
      // new attemptId, say taskId.1) it will be able to acquire lease on the log file (as output stream was
      // closed properly by taskId.0).
      //
      // If close() call were to fail throwing an exception, our best bet is to rollover to a new log file.
      try {
<span class="nc" id="L293">        close();</span>
        // output stream has been successfully closed and lease on the log file has been released,
        // before throwing an exception for the append failure.
<span class="nc" id="L296">        throw new HoodieIOException(&quot;Failed to append to the output stream &quot;, e);</span>
<span class="nc" id="L297">      } catch (Exception ce) {</span>
<span class="nc" id="L298">        LOG.warn(&quot;Failed to close the output stream for &quot; + fs.getClass().getName() + &quot; on path &quot; + path</span>
            + &quot;. Rolling over to a new log file.&quot;);
<span class="nc" id="L300">        this.logFile = logFile.rollOver(fs, rolloverLogWriteToken);</span>
<span class="nc" id="L301">        createNewFile();</span>
      }
    }
<span class="nc" id="L304">  }</span>

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>