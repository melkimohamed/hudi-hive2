<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>CleanHelper.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.table</a> &gt; <span class="el_source">CleanHelper.java</span></div><h1>CleanHelper.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.table;

import org.apache.hudi.avro.model.HoodieCleanMetadata;
import org.apache.hudi.common.model.CompactionOperation;
import org.apache.hudi.common.model.FileSlice;
import org.apache.hudi.common.model.HoodieCleaningPolicy;
import org.apache.hudi.common.model.HoodieCommitMetadata;
import org.apache.hudi.common.model.HoodieBaseFile;
import org.apache.hudi.common.model.HoodieFileGroup;
import org.apache.hudi.common.model.HoodieFileGroupId;
import org.apache.hudi.common.model.HoodieLogFile;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.model.HoodieTableType;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.table.SyncableFileSystemView;
import org.apache.hudi.common.table.timeline.HoodieInstant;
import org.apache.hudi.common.util.AvroUtils;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.common.util.collection.Pair;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.HoodieIOException;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;

import java.io.IOException;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * Cleaner is responsible for garbage collecting older files in a given partition path. Such that
 * &lt;p&gt;
 * 1) It provides sufficient time for existing queries running on older versions, to close
 * &lt;p&gt;
 * 2) It bounds the growth of the files in the file system
 * &lt;p&gt;
 * TODO: Should all cleaning be done based on {@link HoodieCommitMetadata}
 */
public class CleanHelper&lt;T extends HoodieRecordPayload&lt;T&gt;&gt; implements Serializable {

<span class="nc" id="L64">  private static final Logger LOG = LogManager.getLogger(CleanHelper.class);</span>

  private final SyncableFileSystemView fileSystemView;
  private final HoodieTimeline commitTimeline;
  private final Map&lt;HoodieFileGroupId, CompactionOperation&gt; fgIdToPendingCompactionOperations;
  private HoodieTable&lt;T&gt; hoodieTable;
  private HoodieWriteConfig config;

<span class="nc" id="L72">  public CleanHelper(HoodieTable&lt;T&gt; hoodieTable, HoodieWriteConfig config) {</span>
<span class="nc" id="L73">    this.hoodieTable = hoodieTable;</span>
<span class="nc" id="L74">    this.fileSystemView = hoodieTable.getHoodieView();</span>
<span class="nc" id="L75">    this.commitTimeline = hoodieTable.getCompletedCommitTimeline();</span>
<span class="nc" id="L76">    this.config = config;</span>
<span class="nc" id="L77">    this.fgIdToPendingCompactionOperations =</span>
<span class="nc" id="L78">        ((SyncableFileSystemView) hoodieTable.getSliceView()).getPendingCompactionOperations()</span>
<span class="nc" id="L79">            .map(entry -&gt; Pair.of(</span>
<span class="nc" id="L80">                new HoodieFileGroupId(entry.getValue().getPartitionPath(), entry.getValue().getFileId()),</span>
<span class="nc" id="L81">                entry.getValue()))</span>
<span class="nc" id="L82">            .collect(Collectors.toMap(Pair::getKey, Pair::getValue));</span>
<span class="nc" id="L83">  }</span>

  /**
   * Returns list of partitions where clean operations needs to be performed.
   *
   * @param newInstantToRetain New instant to be retained after this cleanup operation
   * @return list of partitions to scan for cleaning
   * @throws IOException when underlying file-system throws this exception
   */
  public List&lt;String&gt; getPartitionPathsToClean(Option&lt;HoodieInstant&gt; newInstantToRetain) throws IOException {
<span class="nc bnc" id="L93" title="All 4 branches missed.">    if (config.incrementalCleanerModeEnabled() &amp;&amp; newInstantToRetain.isPresent()</span>
<span class="nc bnc" id="L94" title="All 2 branches missed.">        &amp;&amp; (HoodieCleaningPolicy.KEEP_LATEST_COMMITS == config.getCleanerPolicy())) {</span>
<span class="nc" id="L95">      Option&lt;HoodieInstant&gt; lastClean =</span>
<span class="nc" id="L96">          hoodieTable.getCleanTimeline().filterCompletedInstants().lastInstant();</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">      if (lastClean.isPresent()) {</span>
<span class="nc" id="L98">        HoodieCleanMetadata cleanMetadata = AvroUtils</span>
<span class="nc" id="L99">            .deserializeHoodieCleanMetadata(hoodieTable.getActiveTimeline().getInstantDetails(lastClean.get()).get());</span>
<span class="nc bnc" id="L100" title="All 2 branches missed.">        if ((cleanMetadata.getEarliestCommitToRetain() != null)</span>
<span class="nc bnc" id="L101" title="All 2 branches missed.">            &amp;&amp; (cleanMetadata.getEarliestCommitToRetain().length() &gt; 0)) {</span>
<span class="nc" id="L102">          LOG.warn(&quot;Incremental Cleaning mode is enabled. Looking up partition-paths that have since changed &quot;</span>
<span class="nc" id="L103">              + &quot;since last cleaned at &quot; + cleanMetadata.getEarliestCommitToRetain()</span>
              + &quot;. New Instant to retain : &quot; + newInstantToRetain);
<span class="nc bnc" id="L105" title="All 2 branches missed.">          return hoodieTable.getCompletedCommitsTimeline().getInstants().filter(instant -&gt; HoodieTimeline.compareTimestamps(instant.getTimestamp(), cleanMetadata.getEarliestCommitToRetain(),</span>
<span class="nc bnc" id="L106" title="All 2 branches missed.">              HoodieTimeline.GREATER_OR_EQUAL) &amp;&amp; HoodieTimeline.compareTimestamps(instant.getTimestamp(),</span>
<span class="nc" id="L107">              newInstantToRetain.get().getTimestamp(), HoodieTimeline.LESSER)).flatMap(instant -&gt; {</span>
                try {
<span class="nc" id="L109">                  HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(hoodieTable.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class);</span>
<span class="nc" id="L110">                  return commitMetadata.getPartitionToWriteStats().keySet().stream();</span>
<span class="nc" id="L111">                } catch (IOException e) {</span>
<span class="nc" id="L112">                  throw new HoodieIOException(e.getMessage(), e);</span>
                }
<span class="nc" id="L114">              }).distinct().collect(Collectors.toList());</span>
        }
      }
    }
    // Otherwise go to brute force mode of scanning all partitions
<span class="nc" id="L119">    return FSUtils.getAllPartitionPaths(hoodieTable.getMetaClient().getFs(),</span>
<span class="nc" id="L120">        hoodieTable.getMetaClient().getBasePath(), config.shouldAssumeDatePartitioning());</span>
  }

  /**
   * Selects the older versions of files for cleaning, such that it bounds the number of versions of each file. This
   * policy is useful, if you are simply interested in querying the table, and you don't want too many versions for a
   * single file (i.e run it with versionsRetained = 1)
   */
  private List&lt;String&gt; getFilesToCleanKeepingLatestVersions(String partitionPath) {
<span class="nc" id="L129">    LOG.info(&quot;Cleaning &quot; + partitionPath + &quot;, retaining latest &quot; + config.getCleanerFileVersionsRetained()</span>
        + &quot; file versions. &quot;);
<span class="nc" id="L131">    List&lt;HoodieFileGroup&gt; fileGroups = fileSystemView.getAllFileGroups(partitionPath).collect(Collectors.toList());</span>
<span class="nc" id="L132">    List&lt;String&gt; deletePaths = new ArrayList&lt;&gt;();</span>
    // Collect all the datafiles savepointed by all the savepoints
<span class="nc" id="L134">    List&lt;String&gt; savepointedFiles = hoodieTable.getSavepoints().stream()</span>
<span class="nc" id="L135">        .flatMap(s -&gt; hoodieTable.getSavepointedDataFiles(s)).collect(Collectors.toList());</span>

<span class="nc bnc" id="L137" title="All 2 branches missed.">    for (HoodieFileGroup fileGroup : fileGroups) {</span>
<span class="nc" id="L138">      int keepVersions = config.getCleanerFileVersionsRetained();</span>
      // do not cleanup slice required for pending compaction
<span class="nc" id="L140">      Iterator&lt;FileSlice&gt; fileSliceIterator =</span>
<span class="nc bnc" id="L141" title="All 2 branches missed.">          fileGroup.getAllFileSlices().filter(fs -&gt; !isFileSliceNeededForPendingCompaction(fs)).iterator();</span>
<span class="nc bnc" id="L142" title="All 2 branches missed.">      if (isFileGroupInPendingCompaction(fileGroup)) {</span>
        // We have already saved the last version of file-groups for pending compaction Id
<span class="nc" id="L144">        keepVersions--;</span>
      }

<span class="nc bnc" id="L147" title="All 4 branches missed.">      while (fileSliceIterator.hasNext() &amp;&amp; keepVersions &gt; 0) {</span>
        // Skip this most recent version
<span class="nc" id="L149">        FileSlice nextSlice = fileSliceIterator.next();</span>
<span class="nc" id="L150">        Option&lt;HoodieBaseFile&gt; dataFile = nextSlice.getBaseFile();</span>
<span class="nc bnc" id="L151" title="All 4 branches missed.">        if (dataFile.isPresent() &amp;&amp; savepointedFiles.contains(dataFile.get().getFileName())) {</span>
          // do not clean up a savepoint data file
<span class="nc" id="L153">          continue;</span>
        }
<span class="nc" id="L155">        keepVersions--;</span>
<span class="nc" id="L156">      }</span>
      // Delete the remaining files
<span class="nc bnc" id="L158" title="All 2 branches missed.">      while (fileSliceIterator.hasNext()) {</span>
<span class="nc" id="L159">        FileSlice nextSlice = fileSliceIterator.next();</span>
<span class="nc bnc" id="L160" title="All 2 branches missed.">        if (nextSlice.getBaseFile().isPresent()) {</span>
<span class="nc" id="L161">          HoodieBaseFile dataFile = nextSlice.getBaseFile().get();</span>
<span class="nc" id="L162">          deletePaths.add(dataFile.getFileName());</span>
        }
<span class="nc bnc" id="L164" title="All 2 branches missed.">        if (hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ) {</span>
          // If merge on read, then clean the log files for the commits as well
<span class="nc" id="L166">          deletePaths.addAll(nextSlice.getLogFiles().map(HoodieLogFile::getFileName).collect(Collectors.toList()));</span>
        }
<span class="nc" id="L168">      }</span>
<span class="nc" id="L169">    }</span>
<span class="nc" id="L170">    return deletePaths;</span>
  }

  /**
   * Selects the versions for file for cleaning, such that it
   * &lt;p&gt;
   * - Leaves the latest version of the file untouched - For older versions, - It leaves all the commits untouched which
   * has occurred in last &lt;code&gt;config.getCleanerCommitsRetained()&lt;/code&gt; commits - It leaves ONE commit before this
   * window. We assume that the max(query execution time) == commit_batch_time * config.getCleanerCommitsRetained().
   * This is 5 hours by default (assuming ingestion is running every 30 minutes). This is essential to leave the file
   * used by the query that is running for the max time.
   * &lt;p&gt;
   * This provides the effect of having lookback into all changes that happened in the last X commits. (eg: if you
   * retain 10 commits, and commit batch time is 30 mins, then you have 5 hrs of lookback)
   * &lt;p&gt;
   * This policy is the default.
   */
  private List&lt;String&gt; getFilesToCleanKeepingLatestCommits(String partitionPath) {
<span class="nc" id="L188">    int commitsRetained = config.getCleanerCommitsRetained();</span>
<span class="nc" id="L189">    LOG.info(&quot;Cleaning &quot; + partitionPath + &quot;, retaining latest &quot; + commitsRetained + &quot; commits. &quot;);</span>
<span class="nc" id="L190">    List&lt;String&gt; deletePaths = new ArrayList&lt;&gt;();</span>

    // Collect all the datafiles savepointed by all the savepoints
<span class="nc" id="L193">    List&lt;String&gt; savepointedFiles = hoodieTable.getSavepoints().stream()</span>
<span class="nc" id="L194">        .flatMap(s -&gt; hoodieTable.getSavepointedDataFiles(s)).collect(Collectors.toList());</span>

    // determine if we have enough commits, to start cleaning.
<span class="nc bnc" id="L197" title="All 2 branches missed.">    if (commitTimeline.countInstants() &gt; commitsRetained) {</span>
<span class="nc" id="L198">      HoodieInstant earliestCommitToRetain = getEarliestCommitToRetain().get();</span>
<span class="nc" id="L199">      List&lt;HoodieFileGroup&gt; fileGroups = fileSystemView.getAllFileGroups(partitionPath).collect(Collectors.toList());</span>
<span class="nc bnc" id="L200" title="All 2 branches missed.">      for (HoodieFileGroup fileGroup : fileGroups) {</span>
<span class="nc" id="L201">        List&lt;FileSlice&gt; fileSliceList = fileGroup.getAllFileSlices().collect(Collectors.toList());</span>

<span class="nc bnc" id="L203" title="All 2 branches missed.">        if (fileSliceList.isEmpty()) {</span>
<span class="nc" id="L204">          continue;</span>
        }

<span class="nc" id="L207">        String lastVersion = fileSliceList.get(0).getBaseInstantTime();</span>
<span class="nc" id="L208">        String lastVersionBeforeEarliestCommitToRetain =</span>
<span class="nc" id="L209">            getLatestVersionBeforeCommit(fileSliceList, earliestCommitToRetain);</span>

        // Ensure there are more than 1 version of the file (we only clean old files from updates)
        // i.e always spare the last commit.
<span class="nc bnc" id="L213" title="All 2 branches missed.">        for (FileSlice aSlice : fileSliceList) {</span>
<span class="nc" id="L214">          Option&lt;HoodieBaseFile&gt; aFile = aSlice.getBaseFile();</span>
<span class="nc" id="L215">          String fileCommitTime = aSlice.getBaseInstantTime();</span>
<span class="nc bnc" id="L216" title="All 4 branches missed.">          if (aFile.isPresent() &amp;&amp; savepointedFiles.contains(aFile.get().getFileName())) {</span>
            // do not clean up a savepoint data file
<span class="nc" id="L218">            continue;</span>
          }
          // Dont delete the latest commit and also the last commit before the earliest commit we
          // are retaining
          // The window of commit retain == max query run time. So a query could be running which
          // still
          // uses this file.
<span class="nc bnc" id="L225" title="All 4 branches missed.">          if (fileCommitTime.equals(lastVersion) || (fileCommitTime.equals(lastVersionBeforeEarliestCommitToRetain))) {</span>
            // move on to the next file
<span class="nc" id="L227">            continue;</span>
          }

          // Always keep the last commit
<span class="nc bnc" id="L231" title="All 2 branches missed.">          if (!isFileSliceNeededForPendingCompaction(aSlice) &amp;&amp; HoodieTimeline</span>
<span class="nc bnc" id="L232" title="All 2 branches missed.">              .compareTimestamps(earliestCommitToRetain.getTimestamp(), fileCommitTime, HoodieTimeline.GREATER)) {</span>
            // this is a commit, that should be cleaned.
<span class="nc" id="L234">            aFile.ifPresent(hoodieDataFile -&gt; deletePaths.add(hoodieDataFile.getFileName()));</span>
<span class="nc bnc" id="L235" title="All 2 branches missed.">            if (hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ) {</span>
              // If merge on read, then clean the log files for the commits as well
<span class="nc" id="L237">              deletePaths.addAll(aSlice.getLogFiles().map(HoodieLogFile::getFileName).collect(Collectors.toList()));</span>
            }
          }
<span class="nc" id="L240">        }</span>
<span class="nc" id="L241">      }</span>
    }

<span class="nc" id="L244">    return deletePaths;</span>
  }

  /**
   * Gets the latest version &lt; commitTime. This version file could still be used by queries.
   */
  private String getLatestVersionBeforeCommit(List&lt;FileSlice&gt; fileSliceList, HoodieInstant commitTime) {
<span class="nc bnc" id="L251" title="All 2 branches missed.">    for (FileSlice file : fileSliceList) {</span>
<span class="nc" id="L252">      String fileCommitTime = file.getBaseInstantTime();</span>
<span class="nc bnc" id="L253" title="All 2 branches missed.">      if (HoodieTimeline.compareTimestamps(commitTime.getTimestamp(), fileCommitTime, HoodieTimeline.GREATER)) {</span>
        // fileList is sorted on the reverse, so the first commit we find &lt;= commitTime is the
        // one we want
<span class="nc" id="L256">        return fileCommitTime;</span>
      }
<span class="nc" id="L258">    }</span>
    // There is no version of this file which is &lt;= commitTime
<span class="nc" id="L260">    return null;</span>
  }

  /**
   * Returns files to be cleaned for the given partitionPath based on cleaning policy.
   */
  public List&lt;String&gt; getDeletePaths(String partitionPath) {
<span class="nc" id="L267">    HoodieCleaningPolicy policy = config.getCleanerPolicy();</span>
    List&lt;String&gt; deletePaths;
<span class="nc bnc" id="L269" title="All 2 branches missed.">    if (policy == HoodieCleaningPolicy.KEEP_LATEST_COMMITS) {</span>
<span class="nc" id="L270">      deletePaths = getFilesToCleanKeepingLatestCommits(partitionPath);</span>
<span class="nc bnc" id="L271" title="All 2 branches missed.">    } else if (policy == HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS) {</span>
<span class="nc" id="L272">      deletePaths = getFilesToCleanKeepingLatestVersions(partitionPath);</span>
    } else {
<span class="nc" id="L274">      throw new IllegalArgumentException(&quot;Unknown cleaning policy : &quot; + policy.name());</span>
    }
<span class="nc" id="L276">    LOG.info(deletePaths.size() + &quot; patterns used to delete in partition path:&quot; + partitionPath);</span>

<span class="nc" id="L278">    return deletePaths;</span>
  }

  /**
   * Returns earliest commit to retain based on cleaning policy.
   */
  public Option&lt;HoodieInstant&gt; getEarliestCommitToRetain() {
<span class="nc" id="L285">    Option&lt;HoodieInstant&gt; earliestCommitToRetain = Option.empty();</span>
<span class="nc" id="L286">    int commitsRetained = config.getCleanerCommitsRetained();</span>
<span class="nc bnc" id="L287" title="All 2 branches missed.">    if (config.getCleanerPolicy() == HoodieCleaningPolicy.KEEP_LATEST_COMMITS</span>
<span class="nc bnc" id="L288" title="All 2 branches missed.">        &amp;&amp; commitTimeline.countInstants() &gt; commitsRetained) {</span>
<span class="nc" id="L289">      earliestCommitToRetain = commitTimeline.nthInstant(commitTimeline.countInstants() - commitsRetained);</span>
    }
<span class="nc" id="L291">    return earliestCommitToRetain;</span>
  }

  /**
   * Determine if file slice needed to be preserved for pending compaction.
   * 
   * @param fileSlice File Slice
   * @return true if file slice needs to be preserved, false otherwise.
   */
  private boolean isFileSliceNeededForPendingCompaction(FileSlice fileSlice) {
<span class="nc" id="L301">    CompactionOperation op = fgIdToPendingCompactionOperations.get(fileSlice.getFileGroupId());</span>
<span class="nc bnc" id="L302" title="All 2 branches missed.">    if (null != op) {</span>
      // If file slice's instant time is newer or same as that of operation, do not clean
<span class="nc" id="L304">      return HoodieTimeline.compareTimestamps(fileSlice.getBaseInstantTime(), op.getBaseInstantTime(),</span>
          HoodieTimeline.GREATER_OR_EQUAL);
    }
<span class="nc" id="L307">    return false;</span>
  }

  private boolean isFileGroupInPendingCompaction(HoodieFileGroup fg) {
<span class="nc" id="L311">    return fgIdToPendingCompactionOperations.containsKey(fg.getFileGroupId());</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>