<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieMergeOnReadTable.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.table</a> &gt; <span class="el_source">HoodieMergeOnReadTable.java</span></div><h1>HoodieMergeOnReadTable.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.table;

import org.apache.hudi.client.WriteStatus;
import org.apache.hudi.avro.model.HoodieCompactionPlan;
import org.apache.hudi.common.HoodieRollbackStat;
import org.apache.hudi.common.model.FileSlice;
import org.apache.hudi.common.model.HoodieCommitMetadata;
import org.apache.hudi.common.model.HoodieLogFile;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.model.HoodieRecordLocation;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.model.HoodieWriteStat;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.table.SyncableFileSystemView;
import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
import org.apache.hudi.common.table.timeline.HoodieInstant;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.HoodieCompactionException;
import org.apache.hudi.exception.HoodieIOException;
import org.apache.hudi.exception.HoodieUpsertException;
import org.apache.hudi.execution.MergeOnReadLazyInsertIterable;
import org.apache.hudi.io.HoodieAppendHandle;
import org.apache.hudi.table.compact.HoodieMergeOnReadTableCompactor;

import com.google.common.base.Preconditions;
import org.apache.hudi.table.rollback.RollbackHelper;
import org.apache.hudi.table.rollback.RollbackRequest;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.Partitioner;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import java.io.IOException;
import java.io.UncheckedIOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.stream.Collectors;

/**
 * Implementation of a more real-time Hoodie Table the provides tradeoffs on read and write cost/amplification.
 *
 * &lt;p&gt;
 * INSERTS - Same as HoodieCopyOnWriteTable - Produce new files, block aligned to desired size (or) Merge with the
 * smallest existing file, to expand it
 * &lt;/p&gt;
 * &lt;p&gt;
 * UPDATES - Appends the changes to a rolling log file maintained per file Id. Compaction merges the log file into the
 * base file.
 * &lt;/p&gt;
 * &lt;p&gt;
 * WARNING - MOR table type does not support nested rollbacks, every rollback must be followed by an attempted commit
 * action
 * &lt;/p&gt;
 */
public class HoodieMergeOnReadTable&lt;T extends HoodieRecordPayload&gt; extends HoodieCopyOnWriteTable&lt;T&gt; {

<span class="nc" id="L82">  private static final Logger LOG = LogManager.getLogger(HoodieMergeOnReadTable.class);</span>

  // UpsertPartitioner for MergeOnRead table type
  private MergeOnReadUpsertPartitioner mergeOnReadUpsertPartitioner;

  public HoodieMergeOnReadTable(HoodieWriteConfig config, JavaSparkContext jsc) {
<span class="nc" id="L88">    super(config, jsc);</span>
<span class="nc" id="L89">  }</span>

  @Override
  public Partitioner getUpsertPartitioner(WorkloadProfile profile) {
<span class="nc bnc" id="L93" title="All 2 branches missed.">    if (profile == null) {</span>
<span class="nc" id="L94">      throw new HoodieUpsertException(&quot;Need workload profile to construct the upsert partitioner.&quot;);</span>
    }
<span class="nc" id="L96">    mergeOnReadUpsertPartitioner = new MergeOnReadUpsertPartitioner(profile);</span>
<span class="nc" id="L97">    return mergeOnReadUpsertPartitioner;</span>
  }

  @Override
  public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpdate(String commitTime, String fileId, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)
      throws IOException {
<span class="nc" id="L103">    LOG.info(&quot;Merging updates for commit &quot; + commitTime + &quot; for file &quot; + fileId);</span>

<span class="nc bnc" id="L105" title="All 4 branches missed.">    if (!index.canIndexLogFiles() &amp;&amp; mergeOnReadUpsertPartitioner.getSmallFileIds().contains(fileId)) {</span>
<span class="nc" id="L106">      LOG.info(&quot;Small file corrections for updates for commit &quot; + commitTime + &quot; for file &quot; + fileId);</span>
<span class="nc" id="L107">      return super.handleUpdate(commitTime, fileId, recordItr);</span>
    } else {
<span class="nc" id="L109">      HoodieAppendHandle&lt;T&gt; appendHandle = new HoodieAppendHandle&lt;&gt;(config, commitTime, this, fileId, recordItr);</span>
<span class="nc" id="L110">      appendHandle.doAppend();</span>
<span class="nc" id="L111">      appendHandle.close();</span>
<span class="nc" id="L112">      return Collections.singletonList(Collections.singletonList(appendHandle.getWriteStatus())).iterator();</span>
    }
  }

  @Override
  public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String commitTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)
      throws Exception {
    // If canIndexLogFiles, write inserts to log files else write inserts to parquet files
<span class="nc bnc" id="L120" title="All 2 branches missed.">    if (index.canIndexLogFiles()) {</span>
<span class="nc" id="L121">      return new MergeOnReadLazyInsertIterable&lt;&gt;(recordItr, config, commitTime, this, idPfx);</span>
    } else {
<span class="nc" id="L123">      return super.handleInsert(commitTime, idPfx, recordItr);</span>
    }
  }

  @Override
  public HoodieCompactionPlan scheduleCompaction(JavaSparkContext jsc, String instantTime) {
<span class="nc" id="L129">    LOG.info(&quot;Checking if compaction needs to be run on &quot; + config.getBasePath());</span>
<span class="nc" id="L130">    Option&lt;HoodieInstant&gt; lastCompaction =</span>
<span class="nc" id="L131">        getActiveTimeline().getCommitTimeline().filterCompletedInstants().lastInstant();</span>
<span class="nc" id="L132">    String deltaCommitsSinceTs = &quot;0&quot;;</span>
<span class="nc bnc" id="L133" title="All 2 branches missed.">    if (lastCompaction.isPresent()) {</span>
<span class="nc" id="L134">      deltaCommitsSinceTs = lastCompaction.get().getTimestamp();</span>
    }

<span class="nc" id="L137">    int deltaCommitsSinceLastCompaction = getActiveTimeline().getDeltaCommitTimeline()</span>
<span class="nc" id="L138">        .findInstantsAfter(deltaCommitsSinceTs, Integer.MAX_VALUE).countInstants();</span>
<span class="nc bnc" id="L139" title="All 2 branches missed.">    if (config.getInlineCompactDeltaCommitMax() &gt; deltaCommitsSinceLastCompaction) {</span>
<span class="nc" id="L140">      LOG.info(&quot;Not running compaction as only &quot; + deltaCommitsSinceLastCompaction</span>
          + &quot; delta commits was found since last compaction &quot; + deltaCommitsSinceTs + &quot;. Waiting for &quot;
<span class="nc" id="L142">          + config.getInlineCompactDeltaCommitMax());</span>
<span class="nc" id="L143">      return new HoodieCompactionPlan();</span>
    }

<span class="nc" id="L146">    LOG.info(&quot;Compacting merge on read table &quot; + config.getBasePath());</span>
<span class="nc" id="L147">    HoodieMergeOnReadTableCompactor compactor = new HoodieMergeOnReadTableCompactor();</span>
    try {
<span class="nc" id="L149">      return compactor.generateCompactionPlan(jsc, this, config, instantTime,</span>
<span class="nc" id="L150">          ((SyncableFileSystemView) getSliceView()).getPendingCompactionOperations()</span>
<span class="nc" id="L151">              .map(instantTimeCompactionopPair -&gt; instantTimeCompactionopPair.getValue().getFileGroupId())</span>
<span class="nc" id="L152">              .collect(Collectors.toSet()));</span>

<span class="nc" id="L154">    } catch (IOException e) {</span>
<span class="nc" id="L155">      throw new HoodieCompactionException(&quot;Could not schedule compaction &quot; + config.getBasePath(), e);</span>
    }
  }

  @Override
  public JavaRDD&lt;WriteStatus&gt; compact(JavaSparkContext jsc, String compactionInstantTime,
      HoodieCompactionPlan compactionPlan) {
<span class="nc" id="L162">    HoodieMergeOnReadTableCompactor compactor = new HoodieMergeOnReadTableCompactor();</span>
    try {
<span class="nc" id="L164">      return compactor.compact(jsc, compactionPlan, this, config, compactionInstantTime);</span>
<span class="nc" id="L165">    } catch (IOException e) {</span>
<span class="nc" id="L166">      throw new HoodieCompactionException(&quot;Could not compact &quot; + config.getBasePath(), e);</span>
    }
  }

  @Override
  public List&lt;HoodieRollbackStat&gt; rollback(JavaSparkContext jsc, HoodieInstant instant,
      boolean deleteInstants) throws IOException {
<span class="nc" id="L173">    long startTime = System.currentTimeMillis();</span>

<span class="nc" id="L175">    String commit = instant.getTimestamp();</span>
<span class="nc" id="L176">    LOG.error(&quot;Rolling back instant &quot; + instant);</span>

    // Atomically un-publish all non-inflight commits
<span class="nc bnc" id="L179" title="All 2 branches missed.">    if (instant.isCompleted()) {</span>
<span class="nc" id="L180">      LOG.error(&quot;Un-publishing instant &quot; + instant + &quot;, deleteInstants=&quot; + deleteInstants);</span>
<span class="nc" id="L181">      instant = this.getActiveTimeline().revertToInflight(instant);</span>
    }

<span class="nc" id="L184">    List&lt;HoodieRollbackStat&gt; allRollbackStats = new ArrayList&lt;&gt;();</span>

    // At the moment, MOR table type does not support bulk nested rollbacks. Nested rollbacks is an experimental
    // feature that is expensive. To perform nested rollbacks, initiate multiple requests of client.rollback
    // (commitToRollback).
    // NOTE {@link HoodieCompactionConfig#withCompactionLazyBlockReadEnabled} needs to be set to TRUE. This is
    // required to avoid OOM when merging multiple LogBlocks performed during nested rollbacks.
    // Atomically un-publish all non-inflight commits
    // Atomically un-publish all non-inflight commits
    // For Requested State (like failure during index lookup), there is nothing to do rollback other than
    // deleting the timeline file
<span class="nc bnc" id="L195" title="All 2 branches missed.">    if (!instant.isRequested()) {</span>
<span class="nc" id="L196">      LOG.info(&quot;Unpublished &quot; + commit);</span>
<span class="nc" id="L197">      List&lt;RollbackRequest&gt; rollbackRequests = generateRollbackRequests(jsc, instant);</span>
      // TODO: We need to persist this as rollback workload and use it in case of partial failures
<span class="nc" id="L199">      allRollbackStats = new RollbackHelper(metaClient, config).performRollback(jsc, instant, rollbackRequests);</span>
    }

    // Delete Inflight instants if enabled
<span class="nc" id="L203">    deleteInflightAndRequestedInstant(deleteInstants, this.getActiveTimeline(), instant);</span>

<span class="nc" id="L205">    LOG.info(&quot;Time(in ms) taken to finish rollback &quot; + (System.currentTimeMillis() - startTime));</span>

<span class="nc" id="L207">    return allRollbackStats;</span>
  }

  /**
   * Generate all rollback requests that we need to perform for rolling back this action without actually performing
   * rolling back.
   * 
   * @param jsc JavaSparkContext
   * @param instantToRollback Instant to Rollback
   * @return list of rollback requests
   * @throws IOException
   */
  private List&lt;RollbackRequest&gt; generateRollbackRequests(JavaSparkContext jsc, HoodieInstant instantToRollback)
      throws IOException {
<span class="nc" id="L221">    String commit = instantToRollback.getTimestamp();</span>
<span class="nc" id="L222">    List&lt;String&gt; partitions = FSUtils.getAllPartitionPaths(this.metaClient.getFs(), this.getMetaClient().getBasePath(),</span>
<span class="nc" id="L223">        config.shouldAssumeDatePartitioning());</span>
<span class="nc" id="L224">    int sparkPartitions = Math.max(Math.min(partitions.size(), config.getRollbackParallelism()), 1);</span>
<span class="nc" id="L225">    return jsc.parallelize(partitions, Math.min(partitions.size(), sparkPartitions)).flatMap(partitionPath -&gt; {</span>
<span class="nc" id="L226">      HoodieActiveTimeline activeTimeline = this.getActiveTimeline().reload();</span>
<span class="nc" id="L227">      List&lt;RollbackRequest&gt; partitionRollbackRequests = new ArrayList&lt;&gt;();</span>
<span class="nc bnc" id="L228" title="All 4 branches missed.">      switch (instantToRollback.getAction()) {</span>
        case HoodieTimeline.COMMIT_ACTION:
<span class="nc" id="L230">          LOG.info(</span>
              &quot;Rolling back commit action. There are higher delta commits. So only rolling back this instant&quot;);
<span class="nc" id="L232">          partitionRollbackRequests.add(</span>
<span class="nc" id="L233">              RollbackRequest.createRollbackRequestWithDeleteDataAndLogFilesAction(partitionPath, instantToRollback));</span>
<span class="nc" id="L234">          break;</span>
        case HoodieTimeline.COMPACTION_ACTION:
          // If there is no delta commit present after the current commit (if compaction), no action, else we
          // need to make sure that a compaction commit rollback also deletes any log files written as part of the
          // succeeding deltacommit.
<span class="nc" id="L239">          boolean higherDeltaCommits =</span>
<span class="nc bnc" id="L240" title="All 2 branches missed.">              !activeTimeline.getDeltaCommitTimeline().filterCompletedInstants().findInstantsAfter(commit, 1).empty();</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">          if (higherDeltaCommits) {</span>
            // Rollback of a compaction action with no higher deltacommit means that the compaction is scheduled
            // and has not yet finished. In this scenario we should delete only the newly created parquet files
            // and not corresponding base commit log files created with this as baseCommit since updates would
            // have been written to the log files.
<span class="nc" id="L246">            LOG.info(&quot;Rolling back compaction. There are higher delta commits. So only deleting data files&quot;);</span>
<span class="nc" id="L247">            partitionRollbackRequests.add(</span>
<span class="nc" id="L248">                RollbackRequest.createRollbackRequestWithDeleteDataFilesOnlyAction(partitionPath, instantToRollback));</span>
          } else {
            // No deltacommits present after this compaction commit (inflight or requested). In this case, we
            // can also delete any log files that were created with this compaction commit as base
            // commit.
<span class="nc" id="L253">            LOG.info(&quot;Rolling back compaction plan. There are NO higher delta commits. So deleting both data and&quot;</span>
                + &quot; log files&quot;);
<span class="nc" id="L255">            partitionRollbackRequests.add(</span>
<span class="nc" id="L256">                RollbackRequest.createRollbackRequestWithDeleteDataAndLogFilesAction(partitionPath, instantToRollback));</span>
          }
<span class="nc" id="L258">          break;</span>
        case HoodieTimeline.DELTA_COMMIT_ACTION:
          // --------------------------------------------------------------------------------------------------
          // (A) The following cases are possible if index.canIndexLogFiles and/or index.isGlobal
          // --------------------------------------------------------------------------------------------------
          // (A.1) Failed first commit - Inserts were written to log files and HoodieWriteStat has no entries. In
          // this scenario we would want to delete these log files.
          // (A.2) Failed recurring commit - Inserts/Updates written to log files. In this scenario,
          // HoodieWriteStat will have the baseCommitTime for the first log file written, add rollback blocks.
          // (A.3) Rollback triggered for first commit - Inserts were written to the log files but the commit is
          // being reverted. In this scenario, HoodieWriteStat will be `null` for the attribute prevCommitTime and
          // and hence will end up deleting these log files. This is done so there are no orphan log files
          // lying around.
          // (A.4) Rollback triggered for recurring commits - Inserts/Updates are being rolled back, the actions
          // taken in this scenario is a combination of (A.2) and (A.3)
          // ---------------------------------------------------------------------------------------------------
          // (B) The following cases are possible if !index.canIndexLogFiles and/or !index.isGlobal
          // ---------------------------------------------------------------------------------------------------
          // (B.1) Failed first commit - Inserts were written to parquet files and HoodieWriteStat has no entries.
          // In this scenario, we delete all the parquet files written for the failed commit.
          // (B.2) Failed recurring commits - Inserts were written to parquet files and updates to log files. In
          // this scenario, perform (A.1) and for updates written to log files, write rollback blocks.
          // (B.3) Rollback triggered for first commit - Same as (B.1)
          // (B.4) Rollback triggered for recurring commits - Same as (B.2) plus we need to delete the log files
          // as well if the base parquet file gets deleted.
          try {
<span class="nc" id="L284">            HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(</span>
<span class="nc" id="L285">                metaClient.getCommitTimeline()</span>
<span class="nc" id="L286">                    .getInstantDetails(</span>
<span class="nc" id="L287">                        new HoodieInstant(true, instantToRollback.getAction(), instantToRollback.getTimestamp()))</span>
<span class="nc" id="L288">                    .get(),</span>
                HoodieCommitMetadata.class);

            // In case all data was inserts and the commit failed, delete the file belonging to that commit
            // We do not know fileIds for inserts (first inserts are either log files or parquet files),
            // delete all files for the corresponding failed commit, if present (same as COW)
<span class="nc" id="L294">            partitionRollbackRequests.add(</span>
<span class="nc" id="L295">                RollbackRequest.createRollbackRequestWithDeleteDataAndLogFilesAction(partitionPath, instantToRollback));</span>

            // append rollback blocks for updates
<span class="nc bnc" id="L298" title="All 2 branches missed.">            if (commitMetadata.getPartitionToWriteStats().containsKey(partitionPath)) {</span>
<span class="nc" id="L299">              partitionRollbackRequests</span>
<span class="nc" id="L300">                  .addAll(generateAppendRollbackBlocksAction(partitionPath, instantToRollback, commitMetadata));</span>
            }
<span class="nc" id="L302">            break;</span>
<span class="nc" id="L303">          } catch (IOException io) {</span>
<span class="nc" id="L304">            throw new UncheckedIOException(&quot;Failed to collect rollback actions for commit &quot; + commit, io);</span>
          }
        default:
          break;
      }
<span class="nc" id="L309">      return partitionRollbackRequests.iterator();</span>
<span class="nc" id="L310">    }).filter(Objects::nonNull).collect();</span>
  }

  @Override
  public void finalizeWrite(JavaSparkContext jsc, String instantTs, List&lt;HoodieWriteStat&gt; stats)
      throws HoodieIOException {
    // delegate to base class for MOR tables
<span class="nc" id="L317">    super.finalizeWrite(jsc, instantTs, stats);</span>
<span class="nc" id="L318">  }</span>

  /**
   * UpsertPartitioner for MergeOnRead table type, this allows auto correction of small parquet files to larger ones
   * without the need for an index in the logFile.
   */
  class MergeOnReadUpsertPartitioner extends HoodieCopyOnWriteTable.UpsertPartitioner {

<span class="nc" id="L326">    MergeOnReadUpsertPartitioner(WorkloadProfile profile) {</span>
<span class="nc" id="L327">      super(profile);</span>
<span class="nc" id="L328">    }</span>

    @Override
    protected List&lt;SmallFile&gt; getSmallFiles(String partitionPath) {

      // smallFiles only for partitionPath
<span class="nc" id="L334">      List&lt;SmallFile&gt; smallFileLocations = new ArrayList&lt;&gt;();</span>

      // Init here since this class (and member variables) might not have been initialized
<span class="nc" id="L337">      HoodieTimeline commitTimeline = getCompletedCommitsTimeline();</span>

      // Find out all eligible small file slices
<span class="nc bnc" id="L340" title="All 2 branches missed.">      if (!commitTimeline.empty()) {</span>
<span class="nc" id="L341">        HoodieInstant latestCommitTime = commitTimeline.lastInstant().get();</span>
        // find smallest file in partition and append to it
<span class="nc" id="L343">        List&lt;FileSlice&gt; allSmallFileSlices = new ArrayList&lt;&gt;();</span>
        // If we cannot index log files, then we choose the smallest parquet file in the partition and add inserts to
        // it. Doing this overtime for a partition, we ensure that we handle small file issues
<span class="nc bnc" id="L346" title="All 2 branches missed.">        if (!index.canIndexLogFiles()) {</span>
          // TODO : choose last N small files since there can be multiple small files written to a single partition
          // by different spark partitions in a single batch
<span class="nc" id="L349">          Option&lt;FileSlice&gt; smallFileSlice = Option.fromJavaOptional(getSliceView()</span>
<span class="nc" id="L350">                  .getLatestFileSlicesBeforeOrOn(partitionPath, latestCommitTime.getTimestamp(), false)</span>
<span class="nc bnc" id="L351" title="All 4 branches missed.">                  .filter(fileSlice -&gt; fileSlice.getLogFiles().count() &lt; 1 &amp;&amp; fileSlice.getBaseFile().get().getFileSize() &lt; config.getParquetSmallFileLimit())</span>
<span class="nc bnc" id="L352" title="All 2 branches missed.">                  .min((FileSlice left, FileSlice right) -&gt; left.getBaseFile().get().getFileSize() &lt; right.getBaseFile().get().getFileSize() ? -1 : 1));</span>
<span class="nc bnc" id="L353" title="All 2 branches missed.">          if (smallFileSlice.isPresent()) {</span>
<span class="nc" id="L354">            allSmallFileSlices.add(smallFileSlice.get());</span>
          }
<span class="nc" id="L356">        } else {</span>
          // If we can index log files, we can add more inserts to log files for fileIds including those under
          // pending compaction.
<span class="nc" id="L359">          List&lt;FileSlice&gt; allFileSlices =</span>
<span class="nc" id="L360">              getSliceView().getLatestFileSlicesBeforeOrOn(partitionPath, latestCommitTime.getTimestamp(), true)</span>
<span class="nc" id="L361">                  .collect(Collectors.toList());</span>
<span class="nc bnc" id="L362" title="All 2 branches missed.">          for (FileSlice fileSlice : allFileSlices) {</span>
<span class="nc bnc" id="L363" title="All 2 branches missed.">            if (isSmallFile(fileSlice)) {</span>
<span class="nc" id="L364">              allSmallFileSlices.add(fileSlice);</span>
            }
<span class="nc" id="L366">          }</span>
        }
        // Create SmallFiles from the eligible file slices
<span class="nc bnc" id="L369" title="All 2 branches missed.">        for (FileSlice smallFileSlice : allSmallFileSlices) {</span>
<span class="nc" id="L370">          SmallFile sf = new SmallFile();</span>
<span class="nc bnc" id="L371" title="All 2 branches missed.">          if (smallFileSlice.getBaseFile().isPresent()) {</span>
            // TODO : Move logic of file name, file id, base commit time handling inside file slice
<span class="nc" id="L373">            String filename = smallFileSlice.getBaseFile().get().getFileName();</span>
<span class="nc" id="L374">            sf.location = new HoodieRecordLocation(FSUtils.getCommitTime(filename), FSUtils.getFileId(filename));</span>
<span class="nc" id="L375">            sf.sizeBytes = getTotalFileSize(smallFileSlice);</span>
<span class="nc" id="L376">            smallFileLocations.add(sf);</span>
            // Update the global small files list
<span class="nc" id="L378">            smallFiles.add(sf);</span>
<span class="nc" id="L379">          } else {</span>
<span class="nc" id="L380">            HoodieLogFile logFile = smallFileSlice.getLogFiles().findFirst().get();</span>
<span class="nc" id="L381">            sf.location = new HoodieRecordLocation(FSUtils.getBaseCommitTimeFromLogPath(logFile.getPath()),</span>
<span class="nc" id="L382">                FSUtils.getFileIdFromLogPath(logFile.getPath()));</span>
<span class="nc" id="L383">            sf.sizeBytes = getTotalFileSize(smallFileSlice);</span>
<span class="nc" id="L384">            smallFileLocations.add(sf);</span>
            // Update the global small files list
<span class="nc" id="L386">            smallFiles.add(sf);</span>
          }
<span class="nc" id="L388">        }</span>
      }
<span class="nc" id="L390">      return smallFileLocations;</span>
    }

    public List&lt;String&gt; getSmallFileIds() {
<span class="nc" id="L394">      return (List&lt;String&gt;) smallFiles.stream().map(smallFile -&gt; ((SmallFile) smallFile).location.getFileId())</span>
<span class="nc" id="L395">          .collect(Collectors.toList());</span>
    }

    private long getTotalFileSize(FileSlice fileSlice) {
<span class="nc bnc" id="L399" title="All 2 branches missed.">      if (!fileSlice.getBaseFile().isPresent()) {</span>
<span class="nc" id="L400">        return convertLogFilesSizeToExpectedParquetSize(fileSlice.getLogFiles().collect(Collectors.toList()));</span>
      } else {
<span class="nc" id="L402">        return fileSlice.getBaseFile().get().getFileSize()</span>
<span class="nc" id="L403">            + convertLogFilesSizeToExpectedParquetSize(fileSlice.getLogFiles().collect(Collectors.toList()));</span>
      }
    }

    private boolean isSmallFile(FileSlice fileSlice) {
<span class="nc" id="L408">      long totalSize = getTotalFileSize(fileSlice);</span>
<span class="nc bnc" id="L409" title="All 2 branches missed.">      return totalSize &lt; config.getParquetMaxFileSize();</span>
    }

    // TODO (NA) : Make this static part of utility
    public long convertLogFilesSizeToExpectedParquetSize(List&lt;HoodieLogFile&gt; hoodieLogFiles) {
<span class="nc" id="L414">      long totalSizeOfLogFiles = hoodieLogFiles.stream().map(HoodieLogFile::getFileSize)</span>
<span class="nc bnc" id="L415" title="All 2 branches missed.">          .filter(size -&gt; size &gt; 0).reduce(Long::sum).orElse(0L);</span>
      // Here we assume that if there is no base parquet file, all log files contain only inserts.
      // We can then just get the parquet equivalent size of these log files, compare that with
      // {@link config.getParquetMaxFileSize()} and decide if there is scope to insert more rows
<span class="nc" id="L419">      return (long) (totalSizeOfLogFiles * config.getLogFileToParquetCompressionRatio());</span>
    }
  }

  private List&lt;RollbackRequest&gt; generateAppendRollbackBlocksAction(String partitionPath, HoodieInstant rollbackInstant,
      HoodieCommitMetadata commitMetadata) {
<span class="nc" id="L425">    Preconditions.checkArgument(rollbackInstant.getAction().equals(HoodieTimeline.DELTA_COMMIT_ACTION));</span>

    // wStat.getPrevCommit() might not give the right commit time in the following
    // scenario : If a compaction was scheduled, the new commitTime associated with the requested compaction will be
    // used to write the new log files. In this case, the commit time for the log file is the compaction requested time.
    // But the index (global) might store the baseCommit of the parquet and not the requested, hence get the
    // baseCommit always by listing the file slice
<span class="nc" id="L432">    Map&lt;String, String&gt; fileIdToBaseCommitTimeForLogMap = this.getSliceView().getLatestFileSlices(partitionPath)</span>
<span class="nc" id="L433">        .collect(Collectors.toMap(FileSlice::getFileId, FileSlice::getBaseInstantTime));</span>
<span class="nc" id="L434">    return commitMetadata.getPartitionToWriteStats().get(partitionPath).stream().filter(wStat -&gt; {</span>

      // Filter out stats without prevCommit since they are all inserts
<span class="nc bnc" id="L437" title="All 4 branches missed.">      boolean validForRollback = (wStat != null) &amp;&amp; (!wStat.getPrevCommit().equals(HoodieWriteStat.NULL_COMMIT))</span>
<span class="nc bnc" id="L438" title="All 4 branches missed.">          &amp;&amp; (wStat.getPrevCommit() != null) &amp;&amp; fileIdToBaseCommitTimeForLogMap.containsKey(wStat.getFileId());</span>

<span class="nc bnc" id="L440" title="All 2 branches missed.">      if (validForRollback) {</span>
        // For sanity, log instant time can never be less than base-commit on which we are rolling back
<span class="nc" id="L442">        Preconditions</span>
<span class="nc" id="L443">            .checkArgument(HoodieTimeline.compareTimestamps(fileIdToBaseCommitTimeForLogMap.get(wStat.getFileId()),</span>
<span class="nc" id="L444">                rollbackInstant.getTimestamp(), HoodieTimeline.LESSER_OR_EQUAL));</span>
      }

<span class="nc bnc" id="L447" title="All 4 branches missed.">      return validForRollback &amp;&amp; HoodieTimeline.compareTimestamps(fileIdToBaseCommitTimeForLogMap.get(</span>
          // Base Ts should be strictly less. If equal (for inserts-to-logs), the caller employs another option
          // to delete and we should not step on it
<span class="nc" id="L450">          wStat.getFileId()), rollbackInstant.getTimestamp(), HoodieTimeline.LESSER);</span>
<span class="nc" id="L451">    }).map(wStat -&gt; {</span>
<span class="nc" id="L452">      String baseCommitTime = fileIdToBaseCommitTimeForLogMap.get(wStat.getFileId());</span>
<span class="nc" id="L453">      return RollbackRequest.createRollbackRequestWithAppendRollbackBlockAction(partitionPath, wStat.getFileId(),</span>
          baseCommitTime, rollbackInstant);
<span class="nc" id="L455">    }).collect(Collectors.toList());</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>