<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieTable.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.table</a> &gt; <span class="el_source">HoodieTable.java</span></div><h1>HoodieTable.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.table;

import org.apache.hudi.client.WriteStatus;
import org.apache.hudi.avro.model.HoodieCleanerPlan;
import org.apache.hudi.avro.model.HoodieCompactionPlan;
import org.apache.hudi.avro.model.HoodieSavepointMetadata;
import org.apache.hudi.client.utils.ClientUtils;
import org.apache.hudi.common.HoodieCleanStat;
import org.apache.hudi.common.HoodieRollbackStat;
import org.apache.hudi.common.SerializableConfiguration;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.model.HoodieWriteStat;
import org.apache.hudi.common.table.HoodieTableMetaClient;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.table.SyncableFileSystemView;
import org.apache.hudi.common.table.TableFileSystemView;
import org.apache.hudi.common.table.TableFileSystemView.BaseFileOnlyView;
import org.apache.hudi.common.table.TableFileSystemView.SliceView;
import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
import org.apache.hudi.common.table.timeline.HoodieInstant;
import org.apache.hudi.common.table.view.FileSystemViewManager;
import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
import org.apache.hudi.common.util.AvroUtils;
import org.apache.hudi.common.util.ConsistencyGuard;
import org.apache.hudi.common.util.ConsistencyGuard.FileVisibility;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.FailSafeConsistencyGuard;
import org.apache.hudi.common.util.collection.Pair;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.HoodieException;
import org.apache.hudi.exception.HoodieIOException;
import org.apache.hudi.exception.HoodieSavepointException;
import org.apache.hudi.index.HoodieIndex;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.Partitioner;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import java.io.IOException;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeoutException;
import java.util.stream.Collectors;
import java.util.stream.Stream;

/**
 * Abstract implementation of a HoodieTable.
 */
public abstract class HoodieTable&lt;T extends HoodieRecordPayload&gt; implements Serializable {

<span class="nc" id="L78">  private static final Logger LOG = LogManager.getLogger(HoodieTable.class);</span>

  protected final HoodieWriteConfig config;
  protected final HoodieTableMetaClient metaClient;
  protected final HoodieIndex&lt;T&gt; index;

  private SerializableConfiguration hadoopConfiguration;
  private transient FileSystemViewManager viewManager;

<span class="nc" id="L87">  protected HoodieTable(HoodieWriteConfig config, JavaSparkContext jsc) {</span>
<span class="nc" id="L88">    this.config = config;</span>
<span class="nc" id="L89">    this.hadoopConfiguration = new SerializableConfiguration(jsc.hadoopConfiguration());</span>
<span class="nc" id="L90">    this.viewManager = FileSystemViewManager.createViewManager(new SerializableConfiguration(jsc.hadoopConfiguration()),</span>
<span class="nc" id="L91">        config.getViewStorageConfig());</span>
<span class="nc" id="L92">    this.metaClient = ClientUtils.createMetaClient(jsc, config, true);</span>
<span class="nc" id="L93">    this.index = HoodieIndex.createIndex(config, jsc);</span>
<span class="nc" id="L94">  }</span>

  private synchronized FileSystemViewManager getViewManager() {
<span class="nc bnc" id="L97" title="All 2 branches missed.">    if (null == viewManager) {</span>
<span class="nc" id="L98">      viewManager = FileSystemViewManager.createViewManager(hadoopConfiguration, config.getViewStorageConfig());</span>
    }
<span class="nc" id="L100">    return viewManager;</span>
  }

  public static &lt;T extends HoodieRecordPayload&gt; HoodieTable&lt;T&gt; getHoodieTable(HoodieTableMetaClient metaClient,
      HoodieWriteConfig config, JavaSparkContext jsc) {
<span class="nc bnc" id="L105" title="All 3 branches missed.">    switch (metaClient.getTableType()) {</span>
      case COPY_ON_WRITE:
<span class="nc" id="L107">        return new HoodieCopyOnWriteTable&lt;&gt;(config, jsc);</span>
      case MERGE_ON_READ:
<span class="nc" id="L109">        return new HoodieMergeOnReadTable&lt;&gt;(config, jsc);</span>
      default:
<span class="nc" id="L111">        throw new HoodieException(&quot;Unsupported table type :&quot; + metaClient.getTableType());</span>
    }
  }

  /**
   * Provides a partitioner to perform the upsert operation, based on the workload profile.
   */
  public abstract Partitioner getUpsertPartitioner(WorkloadProfile profile);

  /**
   * Provides a partitioner to perform the insert operation, based on the workload profile.
   */
  public abstract Partitioner getInsertPartitioner(WorkloadProfile profile);

  /**
   * Return whether this HoodieTable implementation can benefit from workload profiling.
   */
  public abstract boolean isWorkloadProfileNeeded();

  public HoodieWriteConfig getConfig() {
<span class="nc" id="L131">    return config;</span>
  }

  public HoodieTableMetaClient getMetaClient() {
<span class="nc" id="L135">    return metaClient;</span>
  }

  public Configuration getHadoopConf() {
<span class="nc" id="L139">    return metaClient.getHadoopConf();</span>
  }

  /**
   * Get the view of the file system for this table.
   */
  public TableFileSystemView getFileSystemView() {
<span class="nc" id="L146">    return new HoodieTableFileSystemView(metaClient, getCompletedCommitsTimeline());</span>
  }

  /**
   * Get the base file only view of the file system for this table.
   */
  public BaseFileOnlyView getBaseFileOnlyView() {
<span class="nc" id="L153">    return getViewManager().getFileSystemView(metaClient.getBasePath());</span>
  }

  /**
   * Get the full view of the file system for this table.
   */
  public SliceView getSliceView() {
<span class="nc" id="L160">    return getViewManager().getFileSystemView(metaClient.getBasePath());</span>
  }

  /**
   * Get complete view of the file system for this table with ability to force sync.
   */
  public SyncableFileSystemView getHoodieView() {
<span class="nc" id="L167">    return getViewManager().getFileSystemView(metaClient.getBasePath());</span>
  }

  /**
   * Get only the completed (no-inflights) commit + deltacommit timeline.
   */
  public HoodieTimeline getCompletedCommitsTimeline() {
<span class="nc" id="L174">    return metaClient.getCommitsTimeline().filterCompletedInstants();</span>
  }

  /**
   * Get only the completed (no-inflights) commit timeline.
   */
  public HoodieTimeline getCompletedCommitTimeline() {
<span class="nc" id="L181">    return metaClient.getCommitTimeline().filterCompletedInstants();</span>
  }

  /**
   * Get only the inflights (no-completed) commit timeline.
   */
  public HoodieTimeline getPendingCommitTimeline() {
<span class="nc" id="L188">    return metaClient.getCommitsTimeline().filterPendingExcludingCompaction();</span>
  }

  /**
   * Get only the completed (no-inflights) clean timeline.
   */
  public HoodieTimeline getCompletedCleanTimeline() {
<span class="nc" id="L195">    return getActiveTimeline().getCleanerTimeline().filterCompletedInstants();</span>
  }

  /**
   * Get clean timeline.
   */
  public HoodieTimeline getCleanTimeline() {
<span class="nc" id="L202">    return getActiveTimeline().getCleanerTimeline();</span>
  }

  /**
   * Get only the completed (no-inflights) savepoint timeline.
   */
  public HoodieTimeline getCompletedSavepointTimeline() {
<span class="nc" id="L209">    return getActiveTimeline().getSavePointTimeline().filterCompletedInstants();</span>
  }

  /**
   * Get the list of savepoints in this table.
   */
  public List&lt;String&gt; getSavepoints() {
<span class="nc" id="L216">    return getCompletedSavepointTimeline().getInstants().map(HoodieInstant::getTimestamp).collect(Collectors.toList());</span>
  }

  /**
   * Get the list of data file names savepointed.
   */
  public Stream&lt;String&gt; getSavepointedDataFiles(String savepointTime) {
<span class="nc bnc" id="L223" title="All 2 branches missed.">    if (!getSavepoints().contains(savepointTime)) {</span>
<span class="nc" id="L224">      throw new HoodieSavepointException(</span>
          &quot;Could not get data files for savepoint &quot; + savepointTime + &quot;. No such savepoint.&quot;);
    }
<span class="nc" id="L227">    HoodieInstant instant = new HoodieInstant(false, HoodieTimeline.SAVEPOINT_ACTION, savepointTime);</span>
    HoodieSavepointMetadata metadata;
    try {
<span class="nc" id="L230">      metadata = AvroUtils.deserializeHoodieSavepointMetadata(getActiveTimeline().getInstantDetails(instant).get());</span>
<span class="nc" id="L231">    } catch (IOException e) {</span>
<span class="nc" id="L232">      throw new HoodieSavepointException(&quot;Could not get savepointed data files for savepoint &quot; + savepointTime, e);</span>
<span class="nc" id="L233">    }</span>
<span class="nc" id="L234">    return metadata.getPartitionMetadata().values().stream().flatMap(s -&gt; s.getSavepointDataFile().stream());</span>
  }

  public HoodieActiveTimeline getActiveTimeline() {
<span class="nc" id="L238">    return metaClient.getActiveTimeline();</span>
  }

  /**
   * Return the index.
   */
  public HoodieIndex&lt;T&gt; getIndex() {
<span class="nc" id="L245">    return index;</span>
  }

  /**
   * Perform the ultimate IO for a given upserted (RDD) partition.
   */
  public abstract Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String commitTime, Integer partition,
      Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordIterator, Partitioner partitioner);

  /**
   * Perform the ultimate IO for a given inserted (RDD) partition.
   */
  public abstract Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsertPartition(String commitTime, Integer partition,
      Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordIterator, Partitioner partitioner);

  /**
   * Schedule compaction for the instant time.
   * 
   * @param jsc Spark Context
   * @param instantTime Instant Time for scheduling compaction
   * @return
   */
  public abstract HoodieCompactionPlan scheduleCompaction(JavaSparkContext jsc, String instantTime);

  /**
   * Run Compaction on the table. Compaction arranges the data so that it is optimized for data access.
   *
   * @param jsc Spark Context
   * @param compactionInstantTime Instant Time
   * @param compactionPlan Compaction Plan
   */
  public abstract JavaRDD&lt;WriteStatus&gt; compact(JavaSparkContext jsc, String compactionInstantTime,
      HoodieCompactionPlan compactionPlan);

  /**
   * Generates list of files that are eligible for cleaning.
   * 
   * @param jsc Java Spark Context
   * @return Cleaner Plan containing list of files to be deleted.
   */
  public abstract HoodieCleanerPlan scheduleClean(JavaSparkContext jsc);

  /**
   * Cleans the files listed in the cleaner plan associated with clean instant.
   * 
   * @param jsc Java Spark Context
   * @param cleanInstant Clean Instant
   * @param cleanerPlan Cleaner Plan
   * @return list of Clean Stats
   */
  public abstract List&lt;HoodieCleanStat&gt; clean(JavaSparkContext jsc, HoodieInstant cleanInstant,
      HoodieCleanerPlan cleanerPlan);

  /**
   * Rollback the (inflight/committed) record changes with the given commit time. Four steps: (1) Atomically unpublish
   * this commit (2) clean indexing data (3) clean new generated parquet files / log blocks (4) Finally, delete
   * .&lt;action&gt;.commit or .&lt;action&gt;.inflight file if deleteInstants = true
   */
  public abstract List&lt;HoodieRollbackStat&gt; rollback(JavaSparkContext jsc, HoodieInstant instant, boolean deleteInstants)
      throws IOException;

  /**
   * Finalize the written data onto storage. Perform any final cleanups.
   *
   * @param jsc Spark Context
   * @param stats List of HoodieWriteStats
   * @throws HoodieIOException if some paths can't be finalized on storage
   */
  public void finalizeWrite(JavaSparkContext jsc, String instantTs, List&lt;HoodieWriteStat&gt; stats)
      throws HoodieIOException {
<span class="nc" id="L315">    cleanFailedWrites(jsc, instantTs, stats, config.getConsistencyGuardConfig().isConsistencyCheckEnabled());</span>
<span class="nc" id="L316">  }</span>

  /**
   * Delete Marker directory corresponding to an instant.
   * 
   * @param instantTs Instant Time
   */
  protected void deleteMarkerDir(String instantTs) {
    try {
<span class="nc" id="L325">      FileSystem fs = getMetaClient().getFs();</span>
<span class="nc" id="L326">      Path markerDir = new Path(metaClient.getMarkerFolderPath(instantTs));</span>
<span class="nc bnc" id="L327" title="All 2 branches missed.">      if (fs.exists(markerDir)) {</span>
        // For append only case, we do not write to marker dir. Hence, the above check
<span class="nc" id="L329">        LOG.info(&quot;Removing marker directory=&quot; + markerDir);</span>
<span class="nc" id="L330">        fs.delete(markerDir, true);</span>
      }
<span class="nc" id="L332">    } catch (IOException ioe) {</span>
<span class="nc" id="L333">      throw new HoodieIOException(ioe.getMessage(), ioe);</span>
<span class="nc" id="L334">    }</span>
<span class="nc" id="L335">  }</span>

  /**
   * Reconciles WriteStats and marker files to detect and safely delete duplicate data files created because of Spark
   * retries.
   *
   * @param jsc Spark Context
   * @param instantTs Instant Timestamp
   * @param stats Hoodie Write Stat
   * @param consistencyCheckEnabled Consistency Check Enabled
   * @throws HoodieIOException
   */
  protected void cleanFailedWrites(JavaSparkContext jsc, String instantTs, List&lt;HoodieWriteStat&gt; stats,
      boolean consistencyCheckEnabled) throws HoodieIOException {
    try {
      // Reconcile marker and data files with WriteStats so that partially written data-files due to failed
      // (but succeeded on retry) tasks are removed.
<span class="nc" id="L352">      String basePath = getMetaClient().getBasePath();</span>
<span class="nc" id="L353">      FileSystem fs = getMetaClient().getFs();</span>
<span class="nc" id="L354">      Path markerDir = new Path(metaClient.getMarkerFolderPath(instantTs));</span>

<span class="nc bnc" id="L356" title="All 2 branches missed.">      if (!fs.exists(markerDir)) {</span>
        // Happens when all writes are appends
<span class="nc" id="L358">        return;</span>
      }

<span class="nc" id="L361">      List&lt;String&gt; invalidDataPaths = FSUtils.getAllDataFilesForMarkers(fs, basePath, instantTs, markerDir.toString());</span>
<span class="nc" id="L362">      List&lt;String&gt; validDataPaths = stats.stream().map(w -&gt; String.format(&quot;%s/%s&quot;, basePath, w.getPath()))</span>
<span class="nc" id="L363">          .filter(p -&gt; p.endsWith(&quot;.parquet&quot;)).collect(Collectors.toList());</span>
      // Contains list of partially created files. These needs to be cleaned up.
<span class="nc" id="L365">      invalidDataPaths.removeAll(validDataPaths);</span>
<span class="nc bnc" id="L366" title="All 2 branches missed.">      if (!invalidDataPaths.isEmpty()) {</span>
<span class="nc" id="L367">        LOG.info(</span>
            &quot;Removing duplicate data files created due to spark retries before committing. Paths=&quot; + invalidDataPaths);
      }

<span class="nc" id="L371">      Map&lt;String, List&lt;Pair&lt;String, String&gt;&gt;&gt; groupByPartition = invalidDataPaths.stream()</span>
<span class="nc" id="L372">          .map(dp -&gt; Pair.of(new Path(dp).getParent().toString(), dp)).collect(Collectors.groupingBy(Pair::getKey));</span>

<span class="nc bnc" id="L374" title="All 2 branches missed.">      if (!groupByPartition.isEmpty()) {</span>
        // Ensure all files in delete list is actually present. This is mandatory for an eventually consistent FS.
        // Otherwise, we may miss deleting such files. If files are not found even after retries, fail the commit
<span class="nc bnc" id="L377" title="All 2 branches missed.">        if (consistencyCheckEnabled) {</span>
          // This will either ensure all files to be deleted are present.
<span class="nc" id="L379">          waitForAllFiles(jsc, groupByPartition, FileVisibility.APPEAR);</span>
        }

        // Now delete partially written files
<span class="nc" id="L383">        jsc.parallelize(new ArrayList&lt;&gt;(groupByPartition.values()), config.getFinalizeWriteParallelism())</span>
<span class="nc" id="L384">            .map(partitionWithFileList -&gt; {</span>
<span class="nc" id="L385">              final FileSystem fileSystem = metaClient.getFs();</span>
<span class="nc" id="L386">              LOG.info(&quot;Deleting invalid data files=&quot; + partitionWithFileList);</span>
<span class="nc bnc" id="L387" title="All 2 branches missed.">              if (partitionWithFileList.isEmpty()) {</span>
<span class="nc" id="L388">                return true;</span>
              }
              // Delete
<span class="nc" id="L391">              partitionWithFileList.stream().map(Pair::getValue).forEach(file -&gt; {</span>
                try {
<span class="nc" id="L393">                  fileSystem.delete(new Path(file), false);</span>
<span class="nc" id="L394">                } catch (IOException e) {</span>
<span class="nc" id="L395">                  throw new HoodieIOException(e.getMessage(), e);</span>
<span class="nc" id="L396">                }</span>
<span class="nc" id="L397">              });</span>

<span class="nc" id="L399">              return true;</span>
<span class="nc" id="L400">            }).collect();</span>

        // Now ensure the deleted files disappear
<span class="nc bnc" id="L403" title="All 2 branches missed.">        if (consistencyCheckEnabled) {</span>
          // This will either ensure all files to be deleted are absent.
<span class="nc" id="L405">          waitForAllFiles(jsc, groupByPartition, FileVisibility.DISAPPEAR);</span>
        }
      }
      // Now delete the marker directory
<span class="nc" id="L409">      deleteMarkerDir(instantTs);</span>
<span class="nc" id="L410">    } catch (IOException ioe) {</span>
<span class="nc" id="L411">      throw new HoodieIOException(ioe.getMessage(), ioe);</span>
<span class="nc" id="L412">    }</span>
<span class="nc" id="L413">  }</span>

  /**
   * Ensures all files passed either appear or disappear.
   * 
   * @param jsc JavaSparkContext
   * @param groupByPartition Files grouped by partition
   * @param visibility Appear/Disappear
   */
  private void waitForAllFiles(JavaSparkContext jsc, Map&lt;String, List&lt;Pair&lt;String, String&gt;&gt;&gt; groupByPartition,
      FileVisibility visibility) {
    // This will either ensure all files to be deleted are present.
<span class="nc" id="L425">    boolean checkPassed =</span>
<span class="nc" id="L426">        jsc.parallelize(new ArrayList&lt;&gt;(groupByPartition.entrySet()), config.getFinalizeWriteParallelism())</span>
<span class="nc" id="L427">            .map(partitionWithFileList -&gt; waitForCondition(partitionWithFileList.getKey(),</span>
<span class="nc" id="L428">                partitionWithFileList.getValue().stream(), visibility))</span>
<span class="nc" id="L429">            .collect().stream().allMatch(x -&gt; x);</span>
<span class="nc bnc" id="L430" title="All 2 branches missed.">    if (!checkPassed) {</span>
<span class="nc" id="L431">      throw new HoodieIOException(&quot;Consistency check failed to ensure all files &quot; + visibility);</span>
    }
<span class="nc" id="L433">  }</span>

  private boolean waitForCondition(String partitionPath, Stream&lt;Pair&lt;String, String&gt;&gt; partitionFilePaths,
      FileVisibility visibility) {
<span class="nc" id="L437">    final FileSystem fileSystem = metaClient.getRawFs();</span>
<span class="nc" id="L438">    List&lt;String&gt; fileList = partitionFilePaths.map(Pair::getValue).collect(Collectors.toList());</span>
    try {
<span class="nc" id="L440">      getFailSafeConsistencyGuard(fileSystem).waitTill(partitionPath, fileList, visibility);</span>
<span class="nc" id="L441">    } catch (IOException | TimeoutException ioe) {</span>
<span class="nc" id="L442">      LOG.error(&quot;Got exception while waiting for files to show up&quot;, ioe);</span>
<span class="nc" id="L443">      return false;</span>
<span class="nc" id="L444">    }</span>
<span class="nc" id="L445">    return true;</span>
  }

  private ConsistencyGuard getFailSafeConsistencyGuard(FileSystem fileSystem) {
<span class="nc" id="L449">    return new FailSafeConsistencyGuard(fileSystem, config.getConsistencyGuardConfig());</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>