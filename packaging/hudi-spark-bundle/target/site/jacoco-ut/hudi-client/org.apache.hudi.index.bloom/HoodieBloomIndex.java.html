<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieBloomIndex.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.index.bloom</a> &gt; <span class="el_source">HoodieBloomIndex.java</span></div><h1>HoodieBloomIndex.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.index.bloom;

import org.apache.hudi.client.WriteStatus;
import org.apache.hudi.common.model.HoodieKey;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.model.HoodieRecordLocation;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.table.timeline.HoodieInstant;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.common.util.collection.Pair;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.MetadataNotFoundException;
import org.apache.hudi.index.HoodieIndex;
import org.apache.hudi.io.HoodieRangeInfoHandle;
import org.apache.hudi.table.HoodieTable;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.Partitioner;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.storage.StorageLevel;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

import scala.Tuple2;

import static java.util.stream.Collectors.groupingBy;
import static java.util.stream.Collectors.mapping;
import static java.util.stream.Collectors.toList;

/**
 * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.
 */
public class HoodieBloomIndex&lt;T extends HoodieRecordPayload&gt; extends HoodieIndex&lt;T&gt; {

<span class="nc" id="L60">  private static final Logger LOG = LogManager.getLogger(HoodieBloomIndex.class);</span>

  public HoodieBloomIndex(HoodieWriteConfig config) {
<span class="nc" id="L63">    super(config);</span>
<span class="nc" id="L64">  }</span>

  @Override
  public JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; tagLocation(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; recordRDD, JavaSparkContext jsc,
                                              HoodieTable&lt;T&gt; hoodieTable) {

    // Step 0: cache the input record RDD
<span class="nc bnc" id="L71" title="All 2 branches missed.">    if (config.getBloomIndexUseCaching()) {</span>
<span class="nc" id="L72">      recordRDD.persist(config.getBloomIndexInputStorageLevel());</span>
    }

    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)
<span class="nc" id="L76">    JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD =</span>
<span class="nc" id="L77">        recordRDD.mapToPair(record -&gt; new Tuple2&lt;&gt;(record.getPartitionPath(), record.getRecordKey()));</span>

    // Lookup indexes for all the partition/recordkey pair
<span class="nc" id="L80">    JavaPairRDD&lt;HoodieKey, HoodieRecordLocation&gt; keyFilenamePairRDD =</span>
<span class="nc" id="L81">        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);</span>

    // Cache the result, for subsequent stages.
<span class="nc bnc" id="L84" title="All 2 branches missed.">    if (config.getBloomIndexUseCaching()) {</span>
<span class="nc" id="L85">      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());</span>
    }
<span class="nc bnc" id="L87" title="All 2 branches missed.">    if (LOG.isDebugEnabled()) {</span>
<span class="nc" id="L88">      long totalTaggedRecords = keyFilenamePairRDD.count();</span>
<span class="nc" id="L89">      LOG.debug(&quot;Number of update records (ones tagged with a fileID): &quot; + totalTaggedRecords);</span>
    }

    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys
    // Cost: 4 sec.
<span class="nc" id="L94">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);</span>

<span class="nc bnc" id="L96" title="All 2 branches missed.">    if (config.getBloomIndexUseCaching()) {</span>
<span class="nc" id="L97">      recordRDD.unpersist(); // unpersist the input Record RDD</span>
<span class="nc" id="L98">      keyFilenamePairRDD.unpersist();</span>
    }
<span class="nc" id="L100">    return taggedRecordRDD;</span>
  }

  /**
   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not
   * found.
   *
   * @param hoodieKeys  keys to lookup
   * @param jsc         spark context
   * @param hoodieTable hoodie table object
   */
  @Override
  public JavaPairRDD&lt;HoodieKey, Option&lt;Pair&lt;String, String&gt;&gt;&gt; fetchRecordLocation(JavaRDD&lt;HoodieKey&gt; hoodieKeys,
                                                                                  JavaSparkContext jsc, HoodieTable&lt;T&gt; hoodieTable) {
<span class="nc" id="L114">    JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD =</span>
<span class="nc" id="L115">        hoodieKeys.mapToPair(key -&gt; new Tuple2&lt;&gt;(key.getPartitionPath(), key.getRecordKey()));</span>

    // Lookup indexes for all the partition/recordkey pair
<span class="nc" id="L118">    JavaPairRDD&lt;HoodieKey, HoodieRecordLocation&gt; recordKeyLocationRDD =</span>
<span class="nc" id="L119">        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);</span>
<span class="nc" id="L120">    JavaPairRDD&lt;HoodieKey, String&gt; keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -&gt; new Tuple2&lt;&gt;(key, null));</span>

<span class="nc" id="L122">    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -&gt; {</span>
      Option&lt;Pair&lt;String, String&gt;&gt; partitionPathFileidPair;
<span class="nc bnc" id="L124" title="All 2 branches missed.">      if (keyLoc._2._2.isPresent()) {</span>
<span class="nc" id="L125">        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));</span>
      } else {
<span class="nc" id="L127">        partitionPathFileidPair = Option.empty();</span>
      }
<span class="nc" id="L129">      return new Tuple2&lt;&gt;(keyLoc._1, partitionPathFileidPair);</span>
    });
  }

  /**
   * Lookup the location for each record key and return the pair&lt;record_key,location&gt; for all record keys already
   * present and drop the record keys if not present.
   */
  private JavaPairRDD&lt;HoodieKey, HoodieRecordLocation&gt; lookupIndex(
      JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD, final JavaSparkContext jsc,
      final HoodieTable hoodieTable) {
    // Obtain records per partition, in the incoming records
<span class="nc" id="L141">    Map&lt;String, Long&gt; recordsPerPartition = partitionRecordKeyPairRDD.countByKey();</span>
<span class="nc" id="L142">    List&lt;String&gt; affectedPartitionPathList = new ArrayList&lt;&gt;(recordsPerPartition.keySet());</span>

    // Step 2: Load all involved files as &lt;Partition, filename&gt; pairs
<span class="nc" id="L145">    List&lt;Tuple2&lt;String, BloomIndexFileInfo&gt;&gt; fileInfoList =</span>
<span class="nc" id="L146">        loadInvolvedFiles(affectedPartitionPathList, jsc, hoodieTable);</span>
<span class="nc" id="L147">    final Map&lt;String, List&lt;BloomIndexFileInfo&gt;&gt; partitionToFileInfo =</span>
<span class="nc" id="L148">        fileInfoList.stream().collect(groupingBy(Tuple2::_1, mapping(Tuple2::_2, toList())));</span>

    // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,
    // that contains it.
<span class="nc" id="L152">    Map&lt;String, Long&gt; comparisonsPerFileGroup =</span>
<span class="nc" id="L153">        computeComparisonsPerFileGroup(recordsPerPartition, partitionToFileInfo, partitionRecordKeyPairRDD);</span>
<span class="nc" id="L154">    int inputParallelism = partitionRecordKeyPairRDD.partitions().size();</span>
<span class="nc" id="L155">    int joinParallelism = Math.max(inputParallelism, config.getBloomIndexParallelism());</span>
<span class="nc" id="L156">    LOG.info(&quot;InputParallelism: ${&quot; + inputParallelism + &quot;}, IndexParallelism: ${&quot;</span>
<span class="nc" id="L157">        + config.getBloomIndexParallelism() + &quot;}&quot;);</span>
<span class="nc" id="L158">    return findMatchingFilesForRecordKeys(partitionToFileInfo, partitionRecordKeyPairRDD, joinParallelism, hoodieTable,</span>
        comparisonsPerFileGroup);
  }

  /**
   * Compute the estimated number of bloom filter comparisons to be performed on each file group.
   */
  private Map&lt;String, Long&gt; computeComparisonsPerFileGroup(final Map&lt;String, Long&gt; recordsPerPartition,
                                                           final Map&lt;String, List&lt;BloomIndexFileInfo&gt;&gt; partitionToFileInfo,
                                                           JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD) {

    Map&lt;String, Long&gt; fileToComparisons;
<span class="nc bnc" id="L170" title="All 2 branches missed.">    if (config.getBloomIndexPruneByRanges()) {</span>
      // we will just try exploding the input and then count to determine comparisons
      // FIX(vc): Only do sampling here and extrapolate?
<span class="nc" id="L173">      fileToComparisons = explodeRecordRDDWithFileComparisons(partitionToFileInfo, partitionRecordKeyPairRDD)</span>
<span class="nc" id="L174">          .mapToPair(t -&gt; t).countByKey();</span>
    } else {
<span class="nc" id="L176">      fileToComparisons = new HashMap&lt;&gt;();</span>
<span class="nc" id="L177">      partitionToFileInfo.forEach((key, value) -&gt; {</span>
<span class="nc bnc" id="L178" title="All 2 branches missed.">        for (BloomIndexFileInfo fileInfo : value) {</span>
          // each file needs to be compared against all the records coming into the partition
<span class="nc" id="L180">          fileToComparisons.put(fileInfo.getFileId(), recordsPerPartition.get(key));</span>
<span class="nc" id="L181">        }</span>
<span class="nc" id="L182">      });</span>
    }
<span class="nc" id="L184">    return fileToComparisons;</span>
  }

  /**
   * Load all involved files as &lt;Partition, filename&gt; pair RDD.
   */
  List&lt;Tuple2&lt;String, BloomIndexFileInfo&gt;&gt; loadInvolvedFiles(List&lt;String&gt; partitions, final JavaSparkContext jsc,
                                                             final HoodieTable hoodieTable) {

    // Obtain the latest data files from all the partitions.
<span class="nc" id="L194">    List&lt;Pair&lt;String, String&gt;&gt; partitionPathFileIDList =</span>
<span class="nc" id="L195">        jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -&gt; {</span>
<span class="nc" id="L196">          Option&lt;HoodieInstant&gt; latestCommitTime =</span>
<span class="nc" id="L197">              hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();</span>
<span class="nc" id="L198">          List&lt;Pair&lt;String, String&gt;&gt; filteredFiles = new ArrayList&lt;&gt;();</span>
<span class="nc bnc" id="L199" title="All 2 branches missed.">          if (latestCommitTime.isPresent()) {</span>
<span class="nc" id="L200">            filteredFiles = hoodieTable.getBaseFileOnlyView()</span>
<span class="nc" id="L201">                .getLatestBaseFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())</span>
<span class="nc" id="L202">                .map(f -&gt; Pair.of(partitionPath, f.getFileId())).collect(toList());</span>
          }
<span class="nc" id="L204">          return filteredFiles.iterator();</span>
<span class="nc" id="L205">        }).collect();</span>

<span class="nc bnc" id="L207" title="All 2 branches missed.">    if (config.getBloomIndexPruneByRanges()) {</span>
      // also obtain file ranges, if range pruning is enabled
<span class="nc" id="L209">      return jsc.parallelize(partitionPathFileIDList, Math.max(partitionPathFileIDList.size(), 1)).mapToPair(pf -&gt; {</span>
        try {
<span class="nc" id="L211">          HoodieRangeInfoHandle&lt;T&gt; rangeInfoHandle = new HoodieRangeInfoHandle&lt;T&gt;(config, hoodieTable, pf);</span>
<span class="nc" id="L212">          String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys();</span>
<span class="nc" id="L213">          return new Tuple2&lt;&gt;(pf.getKey(), new BloomIndexFileInfo(pf.getValue(), minMaxKeys[0], minMaxKeys[1]));</span>
<span class="nc" id="L214">        } catch (MetadataNotFoundException me) {</span>
<span class="nc" id="L215">          LOG.warn(&quot;Unable to find range metadata in file :&quot; + pf);</span>
<span class="nc" id="L216">          return new Tuple2&lt;&gt;(pf.getKey(), new BloomIndexFileInfo(pf.getValue()));</span>
        }
<span class="nc" id="L218">      }).collect();</span>
    } else {
<span class="nc" id="L220">      return partitionPathFileIDList.stream()</span>
<span class="nc" id="L221">          .map(pf -&gt; new Tuple2&lt;&gt;(pf.getKey(), new BloomIndexFileInfo(pf.getValue()))).collect(toList());</span>
    }
  }

  @Override
  public boolean rollbackCommit(String commitTime) {
    // Nope, don't need to do anything.
<span class="nc" id="L228">    return true;</span>
  }

  /**
   * This is not global, since we depend on the partitionPath to do the lookup.
   */
  @Override
  public boolean isGlobal() {
<span class="nc" id="L236">    return false;</span>
  }

  /**
   * No indexes into log files yet.
   */
  @Override
  public boolean canIndexLogFiles() {
<span class="nc" id="L244">    return false;</span>
  }

  /**
   * Bloom filters are stored, into the same data files.
   */
  @Override
  public boolean isImplicitWithStorage() {
<span class="nc" id="L252">    return true;</span>
  }

  /**
   * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be
   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files
   * to be compared gets cut down a lot from range pruning.
   * &lt;p&gt;
   * Sub-partition to ensure the records can be looked up against files &amp; also prune file&lt;=&gt;record comparisons based on
   * recordKey ranges in the index info.
   */
  JavaRDD&lt;Tuple2&lt;String, HoodieKey&gt;&gt; explodeRecordRDDWithFileComparisons(
      final Map&lt;String, List&lt;BloomIndexFileInfo&gt;&gt; partitionToFileIndexInfo,
      JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD) {
<span class="nc" id="L266">    IndexFileFilter indexFileFilter =</span>
<span class="nc bnc" id="L267" title="All 2 branches missed.">        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)</span>
            : new ListBasedIndexFileFilter(partitionToFileIndexInfo);

<span class="nc" id="L270">    return partitionRecordKeyPairRDD.map(partitionRecordKeyPair -&gt; {</span>
<span class="nc" id="L271">      String recordKey = partitionRecordKeyPair._2();</span>
<span class="nc" id="L272">      String partitionPath = partitionRecordKeyPair._1();</span>

<span class="nc" id="L274">      return indexFileFilter.getMatchingFilesAndPartition(partitionPath, recordKey).stream()</span>
<span class="nc" id="L275">          .map(partitionFileIdPair -&gt; new Tuple2&lt;&gt;(partitionFileIdPair.getRight(),</span>
              new HoodieKey(recordKey, partitionPath)))
<span class="nc" id="L277">          .collect(Collectors.toList());</span>
<span class="nc" id="L278">    }).flatMap(List::iterator);</span>
  }

  /**
   * Find out &lt;RowKey, filename&gt; pair. All workload grouped by file-level.
   * &lt;p&gt;
   * Join PairRDD(PartitionPath, RecordKey) and PairRDD(PartitionPath, File) &amp; then repartition such that each RDD
   * partition is a file, then for each file, we do (1) load bloom filter, (2) load rowKeys, (3) Tag rowKey
   * &lt;p&gt;
   * Make sure the parallelism is atleast the groupby parallelism for tagging location
   */
  JavaPairRDD&lt;HoodieKey, HoodieRecordLocation&gt; findMatchingFilesForRecordKeys(
      final Map&lt;String, List&lt;BloomIndexFileInfo&gt;&gt; partitionToFileIndexInfo,
      JavaPairRDD&lt;String, String&gt; partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,
      Map&lt;String, Long&gt; fileGroupToComparisons) {
<span class="nc" id="L293">    JavaRDD&lt;Tuple2&lt;String, HoodieKey&gt;&gt; fileComparisonsRDD =</span>
<span class="nc" id="L294">        explodeRecordRDDWithFileComparisons(partitionToFileIndexInfo, partitionRecordKeyPairRDD);</span>

<span class="nc bnc" id="L296" title="All 2 branches missed.">    if (config.useBloomIndexBucketizedChecking()) {</span>
<span class="nc" id="L297">      Partitioner partitioner = new BucketizedBloomCheckPartitioner(shuffleParallelism, fileGroupToComparisons,</span>
<span class="nc" id="L298">          config.getBloomIndexKeysPerBucket());</span>

<span class="nc" id="L300">      fileComparisonsRDD = fileComparisonsRDD.mapToPair(t -&gt; new Tuple2&lt;&gt;(Pair.of(t._1, t._2.getRecordKey()), t))</span>
<span class="nc" id="L301">          .repartitionAndSortWithinPartitions(partitioner).map(Tuple2::_2);</span>
<span class="nc" id="L302">    } else {</span>
<span class="nc" id="L303">      fileComparisonsRDD = fileComparisonsRDD.sortBy(Tuple2::_1, true, shuffleParallelism);</span>
    }

<span class="nc" id="L306">    return fileComparisonsRDD.mapPartitionsWithIndex(new HoodieBloomIndexCheckFunction(hoodieTable, config), true)</span>
<span class="nc bnc" id="L307" title="All 2 branches missed.">        .flatMap(List::iterator).filter(lr -&gt; lr.getMatchingRecordKeys().size() &gt; 0)</span>
<span class="nc" id="L308">        .flatMapToPair(lookupResult -&gt; lookupResult.getMatchingRecordKeys().stream()</span>
<span class="nc" id="L309">            .map(recordKey -&gt; new Tuple2&lt;&gt;(new HoodieKey(recordKey, lookupResult.getPartitionPath()),</span>
<span class="nc" id="L310">                new HoodieRecordLocation(lookupResult.getBaseInstantTime(), lookupResult.getFileId())))</span>
<span class="nc" id="L311">            .collect(Collectors.toList()).iterator());</span>
  }

  HoodieRecord&lt;T&gt; getTaggedRecord(HoodieRecord&lt;T&gt; inputRecord, Option&lt;HoodieRecordLocation&gt; location) {
<span class="nc" id="L315">    HoodieRecord&lt;T&gt; record = inputRecord;</span>
<span class="nc bnc" id="L316" title="All 2 branches missed.">    if (location.isPresent()) {</span>
      // When you have a record in multiple files in the same partition, then rowKeyRecordPairRDD
      // will have 2 entries with the same exact in memory copy of the HoodieRecord and the 2
      // separate filenames that the record is found in. This will result in setting
      // currentLocation 2 times and it will fail the second time. So creating a new in memory
      // copy of the hoodie record.
<span class="nc" id="L322">      record = new HoodieRecord&lt;&gt;(inputRecord);</span>
<span class="nc" id="L323">      record.unseal();</span>
<span class="nc" id="L324">      record.setCurrentLocation(location.get());</span>
<span class="nc" id="L325">      record.seal();</span>
    }
<span class="nc" id="L327">    return record;</span>
  }

  /**
   * Tag the &lt;rowKey, filename&gt; back to the original HoodieRecord RDD.
   */
  protected JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; tagLocationBacktoRecords(
      JavaPairRDD&lt;HoodieKey, HoodieRecordLocation&gt; keyFilenamePairRDD, JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; recordRDD) {
<span class="nc" id="L335">    JavaPairRDD&lt;HoodieKey, HoodieRecord&lt;T&gt;&gt; keyRecordPairRDD =</span>
<span class="nc" id="L336">        recordRDD.mapToPair(record -&gt; new Tuple2&lt;&gt;(record.getKey(), record));</span>
    // Here as the recordRDD might have more data than rowKeyRDD (some rowKeys' fileId is null),
    // so we do left outer join.
<span class="nc" id="L339">    return keyRecordPairRDD.leftOuterJoin(keyFilenamePairRDD).values()</span>
<span class="nc" id="L340">        .map(v1 -&gt; getTaggedRecord(v1._1, Option.ofNullable(v1._2.orNull())));</span>
  }

  @Override
  public JavaRDD&lt;WriteStatus&gt; updateLocation(JavaRDD&lt;WriteStatus&gt; writeStatusRDD, JavaSparkContext jsc,
                                             HoodieTable&lt;T&gt; hoodieTable) {
<span class="nc" id="L346">    return writeStatusRDD;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>