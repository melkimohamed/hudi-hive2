<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieMergeOnReadTableCompactor.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.table.compact</a> &gt; <span class="el_source">HoodieMergeOnReadTableCompactor.java</span></div><h1>HoodieMergeOnReadTableCompactor.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.table.compact;

import org.apache.hudi.client.WriteStatus;
import org.apache.hudi.avro.model.HoodieCompactionOperation;
import org.apache.hudi.avro.model.HoodieCompactionPlan;
import org.apache.hudi.common.model.CompactionOperation;
import org.apache.hudi.common.model.HoodieBaseFile;
import org.apache.hudi.common.model.HoodieFileGroupId;
import org.apache.hudi.common.model.HoodieLogFile;
import org.apache.hudi.common.model.HoodieTableType;
import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
import org.apache.hudi.common.table.HoodieTableMetaClient;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.table.TableFileSystemView.SliceView;
import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;
import org.apache.hudi.common.util.CompactionUtils;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.HoodieAvroUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.common.util.collection.Pair;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.table.compact.strategy.CompactionStrategy;
import org.apache.hudi.table.HoodieCopyOnWriteTable;
import org.apache.hudi.table.HoodieTable;

import com.google.common.base.Preconditions;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import org.apache.avro.Schema;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.util.AccumulatorV2;
import org.apache.spark.util.LongAccumulator;

import java.io.IOException;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
import java.util.stream.Collectors;
import java.util.stream.StreamSupport;

import static java.util.stream.Collectors.toList;

/**
 * Compacts a hoodie table with merge on read storage. Computes all possible compactions,
 * passes it through a CompactionFilter and executes all the compactions and writes a new version of base files and make
 * a normal commit
 *
 * @see HoodieCompactor
 */
<span class="nc" id="L75">public class HoodieMergeOnReadTableCompactor implements HoodieCompactor {</span>

<span class="nc" id="L77">  private static final Logger LOG = LogManager.getLogger(HoodieMergeOnReadTableCompactor.class);</span>
  // Accumulator to keep track of total log files for a table
  private AccumulatorV2&lt;Long, Long&gt; totalLogFiles;
  // Accumulator to keep track of total log file slices for a table
  private AccumulatorV2&lt;Long, Long&gt; totalFileSlices;

  @Override
  public JavaRDD&lt;WriteStatus&gt; compact(JavaSparkContext jsc, HoodieCompactionPlan compactionPlan,
      HoodieTable hoodieTable, HoodieWriteConfig config, String compactionInstantTime) throws IOException {
<span class="nc bnc" id="L86" title="All 4 branches missed.">    if (compactionPlan == null || (compactionPlan.getOperations() == null)</span>
<span class="nc bnc" id="L87" title="All 2 branches missed.">        || (compactionPlan.getOperations().isEmpty())) {</span>
<span class="nc" id="L88">      return jsc.emptyRDD();</span>
    }
<span class="nc" id="L90">    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();</span>
    // Compacting is very similar to applying updates to existing file
<span class="nc" id="L92">    HoodieCopyOnWriteTable table = new HoodieCopyOnWriteTable(config, jsc);</span>
<span class="nc" id="L93">    List&lt;CompactionOperation&gt; operations = compactionPlan.getOperations().stream()</span>
<span class="nc" id="L94">        .map(CompactionOperation::convertFromAvroRecordInstance).collect(toList());</span>
<span class="nc" id="L95">    LOG.info(&quot;Compactor compacting &quot; + operations + &quot; files&quot;);</span>

<span class="nc" id="L97">    return jsc.parallelize(operations, operations.size())</span>
<span class="nc" id="L98">        .map(s -&gt; compact(table, metaClient, config, s, compactionInstantTime)).flatMap(List::iterator);</span>
  }

  private List&lt;WriteStatus&gt; compact(HoodieCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,
      HoodieWriteConfig config, CompactionOperation operation, String commitTime) throws IOException {
<span class="nc" id="L103">    FileSystem fs = metaClient.getFs();</span>

<span class="nc" id="L105">    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));</span>
<span class="nc" id="L106">    LOG.info(&quot;Compacting base &quot; + operation.getDataFileName() + &quot; with delta files &quot; + operation.getDeltaFileNames()</span>
        + &quot; for commit &quot; + commitTime);
    // TODO - FIX THIS
    // Reads the entire avro file. Always only specific blocks should be read from the avro file
    // (failure recover).
    // Load all the delta commits since the last compaction commit and get all the blocks to be
    // loaded and load it using CompositeAvroLogReader
    // Since a DeltaCommit is not defined yet, reading all the records. revisit this soon.
<span class="nc" id="L114">    String maxInstantTime = metaClient</span>
<span class="nc" id="L115">        .getActiveTimeline().getTimelineOfActions(Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,</span>
            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))
<span class="nc" id="L117">        .filterCompletedInstants().lastInstant().get().getTimestamp();</span>
<span class="nc" id="L118">    LOG.info(&quot;MaxMemoryPerCompaction =&gt; &quot; + config.getMaxMemoryPerCompaction());</span>

<span class="nc" id="L120">    List&lt;String&gt; logFiles = operation.getDeltaFileNames().stream().map(</span>
<span class="nc" id="L121">        p -&gt; new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())</span>
<span class="nc" id="L122">        .collect(toList());</span>
<span class="nc" id="L123">    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, metaClient.getBasePath(), logFiles,</span>
<span class="nc" id="L124">        readerSchema, maxInstantTime, config.getMaxMemoryPerCompaction(), config.getCompactionLazyBlockReadEnabled(),</span>
<span class="nc" id="L125">        config.getCompactionReverseLogReadEnabled(), config.getMaxDFSStreamBufferSize(),</span>
<span class="nc" id="L126">        config.getSpillableMapBasePath());</span>
<span class="nc bnc" id="L127" title="All 2 branches missed.">    if (!scanner.iterator().hasNext()) {</span>
<span class="nc" id="L128">      return Lists.&lt;WriteStatus&gt;newArrayList();</span>
    }

<span class="nc" id="L131">    Option&lt;HoodieBaseFile&gt; oldDataFileOpt =</span>
<span class="nc" id="L132">        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());</span>

    // Compacting is very similar to applying updates to existing file
    Iterator&lt;List&lt;WriteStatus&gt;&gt; result;
    // If the dataFile is present, there is a base parquet file present, perform updates else perform inserts into a
    // new base parquet file.
<span class="nc bnc" id="L138" title="All 2 branches missed.">    if (oldDataFileOpt.isPresent()) {</span>
<span class="nc" id="L139">      result = hoodieCopyOnWriteTable.handleUpdate(commitTime, operation.getFileId(), scanner.getRecords(),</span>
<span class="nc" id="L140">          oldDataFileOpt.get());</span>
    } else {
<span class="nc" id="L142">      result = hoodieCopyOnWriteTable.handleInsert(commitTime, operation.getPartitionPath(), operation.getFileId(),</span>
<span class="nc" id="L143">          scanner.iterator());</span>
    }
<span class="nc" id="L145">    Iterable&lt;List&lt;WriteStatus&gt;&gt; resultIterable = () -&gt; result;</span>
<span class="nc" id="L146">    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -&gt; {</span>
<span class="nc" id="L147">      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());</span>
<span class="nc" id="L148">      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());</span>
<span class="nc" id="L149">      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());</span>
<span class="nc" id="L150">      s.getStat().setPartitionPath(operation.getPartitionPath());</span>
<span class="nc" id="L151">      s.getStat()</span>
<span class="nc" id="L152">          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());</span>
<span class="nc" id="L153">      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());</span>
<span class="nc" id="L154">      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());</span>
<span class="nc" id="L155">      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());</span>
<span class="nc" id="L156">      RuntimeStats runtimeStats = new RuntimeStats();</span>
<span class="nc" id="L157">      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());</span>
<span class="nc" id="L158">      s.getStat().setRuntimeStats(runtimeStats);</span>
<span class="nc" id="L159">    }).collect(toList());</span>
  }

  @Override
  public HoodieCompactionPlan generateCompactionPlan(JavaSparkContext jsc, HoodieTable hoodieTable,
      HoodieWriteConfig config, String compactionCommitTime, Set&lt;HoodieFileGroupId&gt; fgIdsInPendingCompactions)
      throws IOException {

<span class="nc" id="L167">    totalLogFiles = new LongAccumulator();</span>
<span class="nc" id="L168">    totalFileSlices = new LongAccumulator();</span>
<span class="nc" id="L169">    jsc.sc().register(totalLogFiles);</span>
<span class="nc" id="L170">    jsc.sc().register(totalFileSlices);</span>

<span class="nc bnc" id="L172" title="All 2 branches missed.">    Preconditions.checkArgument(hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ,</span>
        &quot;Can only compact table of type &quot; + HoodieTableType.MERGE_ON_READ + &quot; and not &quot;
<span class="nc" id="L174">            + hoodieTable.getMetaClient().getTableType().name());</span>

    // TODO : check if maxMemory is not greater than JVM or spark.executor memory
    // TODO - rollback any compactions in flight
<span class="nc" id="L178">    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();</span>
<span class="nc" id="L179">    LOG.info(&quot;Compacting &quot; + metaClient.getBasePath() + &quot; with commit &quot; + compactionCommitTime);</span>
<span class="nc" id="L180">    List&lt;String&gt; partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),</span>
<span class="nc" id="L181">        config.shouldAssumeDatePartitioning());</span>

    // filter the partition paths if needed to reduce list status
<span class="nc" id="L184">    partitionPaths = config.getCompactionStrategy().filterPartitionPaths(config, partitionPaths);</span>

<span class="nc bnc" id="L186" title="All 2 branches missed.">    if (partitionPaths.isEmpty()) {</span>
      // In case no partitions could be picked, return no compaction plan
<span class="nc" id="L188">      return null;</span>
    }

<span class="nc" id="L191">    SliceView fileSystemView = hoodieTable.getSliceView();</span>
<span class="nc" id="L192">    LOG.info(&quot;Compaction looking for files to compact in &quot; + partitionPaths + &quot; partitions&quot;);</span>
<span class="nc" id="L193">    List&lt;HoodieCompactionOperation&gt; operations = jsc.parallelize(partitionPaths, partitionPaths.size())</span>
<span class="nc" id="L194">        .flatMap((FlatMapFunction&lt;String, CompactionOperation&gt;) partitionPath -&gt; fileSystemView</span>
<span class="nc" id="L195">            .getLatestFileSlices(partitionPath)</span>
<span class="nc bnc" id="L196" title="All 2 branches missed.">            .filter(slice -&gt; !fgIdsInPendingCompactions.contains(slice.getFileGroupId())).map(s -&gt; {</span>
<span class="nc" id="L197">              List&lt;HoodieLogFile&gt; logFiles =</span>
<span class="nc" id="L198">                  s.getLogFiles().sorted(HoodieLogFile.getLogFileComparator()).collect(Collectors.toList());</span>
<span class="nc" id="L199">              totalLogFiles.add((long) logFiles.size());</span>
<span class="nc" id="L200">              totalFileSlices.add(1L);</span>
              // Avro generated classes are not inheriting Serializable. Using CompactionOperation POJO
              // for spark Map operations and collecting them finally in Avro generated classes for storing
              // into meta files.
<span class="nc" id="L204">              Option&lt;HoodieBaseFile&gt; dataFile = s.getBaseFile();</span>
<span class="nc" id="L205">              return new CompactionOperation(dataFile, partitionPath, logFiles,</span>
<span class="nc" id="L206">                  config.getCompactionStrategy().captureMetrics(config, dataFile, partitionPath, logFiles));</span>
<span class="nc bnc" id="L207" title="All 2 branches missed.">            }).filter(c -&gt; !c.getDeltaFileNames().isEmpty()).collect(toList()).iterator())</span>
<span class="nc" id="L208">        .collect().stream().map(CompactionUtils::buildHoodieCompactionOperation).collect(toList());</span>
<span class="nc" id="L209">    LOG.info(&quot;Total of &quot; + operations.size() + &quot; compactions are retrieved&quot;);</span>
<span class="nc" id="L210">    LOG.info(&quot;Total number of latest files slices &quot; + totalFileSlices.value());</span>
<span class="nc" id="L211">    LOG.info(&quot;Total number of log files &quot; + totalLogFiles.value());</span>
<span class="nc" id="L212">    LOG.info(&quot;Total number of file slices &quot; + totalFileSlices.value());</span>
    // Filter the compactions with the passed in filter. This lets us choose most effective
    // compactions only
<span class="nc" id="L215">    HoodieCompactionPlan compactionPlan = config.getCompactionStrategy().generateCompactionPlan(config, operations,</span>
<span class="nc" id="L216">        CompactionUtils.getAllPendingCompactionPlans(metaClient).stream().map(Pair::getValue).collect(toList()));</span>
<span class="nc" id="L217">    Preconditions.checkArgument(</span>
<span class="nc" id="L218">        compactionPlan.getOperations().stream().noneMatch(</span>
<span class="nc" id="L219">            op -&gt; fgIdsInPendingCompactions.contains(new HoodieFileGroupId(op.getPartitionPath(), op.getFileId()))),</span>
        &quot;Bad Compaction Plan. FileId MUST NOT have multiple pending compactions. &quot;
            + &quot;Please fix your strategy implementation. FileIdsWithPendingCompactions :&quot; + fgIdsInPendingCompactions
            + &quot;, Selected workload :&quot; + compactionPlan);
<span class="nc bnc" id="L223" title="All 2 branches missed.">    if (compactionPlan.getOperations().isEmpty()) {</span>
<span class="nc" id="L224">      LOG.warn(&quot;After filtering, Nothing to compact for &quot; + metaClient.getBasePath());</span>
    }
<span class="nc" id="L226">    return compactionPlan;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>