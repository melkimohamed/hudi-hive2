<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieReadClient.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.client</a> &gt; <span class="el_source">HoodieReadClient.java</span></div><h1>HoodieReadClient.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.client;

import org.apache.hudi.avro.model.HoodieCompactionPlan;
import org.apache.hudi.client.embedded.EmbeddedTimelineService;
import org.apache.hudi.common.model.HoodieBaseFile;
import org.apache.hudi.common.model.HoodieKey;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.table.HoodieTableMetaClient;
import org.apache.hudi.common.table.HoodieTimeline;
import org.apache.hudi.common.util.CompactionUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.common.util.collection.Pair;
import org.apache.hudi.config.HoodieIndexConfig;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.HoodieIndexException;
import org.apache.hudi.index.HoodieIndex;
import org.apache.hudi.table.HoodieTable;

import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.StructType;

import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.stream.Collectors;

import scala.Tuple2;

/**
 * Provides an RDD based API for accessing/filtering Hoodie tables, based on keys.
 */
public class HoodieReadClient&lt;T extends HoodieRecordPayload&gt; extends AbstractHoodieClient {

<span class="nc" id="L61">  private static final Logger LOG = LogManager.getLogger(HoodieReadClient.class);</span>

  /**
   * TODO: We need to persist the index type into hoodie.properties and be able to access the index just with a simple
   * basepath pointing to the table. Until, then just always assume a BloomIndex
   */
  private final transient HoodieIndex&lt;T&gt; index;
  private final HoodieTimeline commitTimeline;
  private HoodieTable hoodieTable;
  private transient Option&lt;SQLContext&gt; sqlContextOpt;

  /**
   * @param basePath path to Hoodie table
   */
  public HoodieReadClient(JavaSparkContext jsc, String basePath, Option&lt;EmbeddedTimelineService&gt; timelineService) {
<span class="nc" id="L76">    this(jsc, HoodieWriteConfig.newBuilder().withPath(basePath)</span>
        // by default we use HoodieBloomIndex
<span class="nc" id="L78">        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build()).build(),</span>
        timelineService);
<span class="nc" id="L80">  }</span>

  /**
   * @param basePath path to Hoodie table
   */
  public HoodieReadClient(JavaSparkContext jsc, String basePath) {
<span class="nc" id="L86">    this(jsc, basePath, Option.empty());</span>
<span class="nc" id="L87">  }</span>

  /**
   * @param jsc
   * @param basePath
   * @param sqlContext
   */
  public HoodieReadClient(JavaSparkContext jsc, String basePath, SQLContext sqlContext) {
<span class="nc" id="L95">    this(jsc, basePath);</span>
<span class="nc" id="L96">    this.sqlContextOpt = Option.of(sqlContext);</span>
<span class="nc" id="L97">  }</span>

  /**
   * @param clientConfig instance of HoodieWriteConfig
   */
  public HoodieReadClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {
<span class="nc" id="L103">    this(jsc, clientConfig, Option.empty());</span>
<span class="nc" id="L104">  }</span>

  /**
   * @param clientConfig instance of HoodieWriteConfig
   */
  public HoodieReadClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig,
      Option&lt;EmbeddedTimelineService&gt; timelineService) {
<span class="nc" id="L111">    super(jsc, clientConfig, timelineService);</span>
<span class="nc" id="L112">    final String basePath = clientConfig.getBasePath();</span>
    // Create a Hoodie table which encapsulated the commits and files visible
<span class="nc" id="L114">    HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), basePath, true);</span>
<span class="nc" id="L115">    this.hoodieTable = HoodieTable.getHoodieTable(metaClient, clientConfig, jsc);</span>
<span class="nc" id="L116">    this.commitTimeline = metaClient.getCommitTimeline().filterCompletedInstants();</span>
<span class="nc" id="L117">    this.index = HoodieIndex.createIndex(clientConfig, jsc);</span>
<span class="nc" id="L118">    this.sqlContextOpt = Option.empty();</span>
<span class="nc" id="L119">  }</span>

  /**
   * Adds support for accessing Hoodie built tables from SparkSQL, as you normally would.
   *
   * @return SparkConf object to be used to construct the SparkContext by caller
   */
  public static SparkConf addHoodieSupport(SparkConf conf) {
<span class="nc" id="L127">    conf.set(&quot;spark.sql.hive.convertMetastoreParquet&quot;, &quot;false&quot;);</span>
<span class="nc" id="L128">    return conf;</span>
  }

  private void assertSqlContext() {
<span class="nc bnc" id="L132" title="All 2 branches missed.">    if (!sqlContextOpt.isPresent()) {</span>
<span class="nc" id="L133">      throw new IllegalStateException(&quot;SQLContext must be set, when performing dataframe operations&quot;);</span>
    }
<span class="nc" id="L135">  }</span>

  private Option&lt;String&gt; convertToDataFilePath(Option&lt;Pair&lt;String, String&gt;&gt; partitionPathFileIDPair) {
<span class="nc bnc" id="L138" title="All 2 branches missed.">    if (partitionPathFileIDPair.isPresent()) {</span>
<span class="nc" id="L139">      HoodieBaseFile dataFile = hoodieTable.getBaseFileOnlyView()</span>
<span class="nc" id="L140">          .getLatestBaseFile(partitionPathFileIDPair.get().getLeft(), partitionPathFileIDPair.get().getRight()).get();</span>
<span class="nc" id="L141">      return Option.of(dataFile.getPath());</span>
    } else {
<span class="nc" id="L143">      return Option.empty();</span>
    }
  }

  /**
   * Given a bunch of hoodie keys, fetches all the individual records out as a data frame.
   *
   * @return a dataframe
   */
  public Dataset&lt;Row&gt; readROView(JavaRDD&lt;HoodieKey&gt; hoodieKeys, int parallelism) {
<span class="nc" id="L153">    assertSqlContext();</span>
<span class="nc" id="L154">    JavaPairRDD&lt;HoodieKey, Option&lt;Pair&lt;String, String&gt;&gt;&gt; lookupResultRDD =</span>
<span class="nc" id="L155">        index.fetchRecordLocation(hoodieKeys, jsc, hoodieTable);</span>
<span class="nc" id="L156">    JavaPairRDD&lt;HoodieKey, Option&lt;String&gt;&gt; keyToFileRDD =</span>
<span class="nc" id="L157">        lookupResultRDD.mapToPair(r -&gt; new Tuple2&lt;&gt;(r._1, convertToDataFilePath(r._2)));</span>
<span class="nc" id="L158">    List&lt;String&gt; paths = keyToFileRDD.filter(keyFileTuple -&gt; keyFileTuple._2().isPresent())</span>
<span class="nc" id="L159">        .map(keyFileTuple -&gt; keyFileTuple._2().get()).collect();</span>

    // record locations might be same for multiple keys, so need a unique list
<span class="nc" id="L162">    Set&lt;String&gt; uniquePaths = new HashSet&lt;&gt;(paths);</span>
<span class="nc" id="L163">    Dataset&lt;Row&gt; originalDF = sqlContextOpt.get().read().parquet(uniquePaths.toArray(new String[uniquePaths.size()]));</span>
<span class="nc" id="L164">    StructType schema = originalDF.schema();</span>
<span class="nc" id="L165">    JavaPairRDD&lt;HoodieKey, Row&gt; keyRowRDD = originalDF.javaRDD().mapToPair(row -&gt; {</span>
<span class="nc" id="L166">      HoodieKey key = new HoodieKey(row.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD),</span>
<span class="nc" id="L167">          row.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD));</span>
<span class="nc" id="L168">      return new Tuple2&lt;&gt;(key, row);</span>
    });

    // Now, we need to further filter out, for only rows that match the supplied hoodie keys
<span class="nc" id="L172">    JavaRDD&lt;Row&gt; rowRDD = keyRowRDD.join(keyToFileRDD, parallelism).map(tuple -&gt; tuple._2()._1());</span>
<span class="nc" id="L173">    return sqlContextOpt.get().createDataFrame(rowRDD, schema);</span>
  }

  /**
   * Checks if the given [Keys] exists in the hoodie table and returns [Key, Option[FullFilePath]] If the optional
   * FullFilePath value is not present, then the key is not found. If the FullFilePath value is present, it is the path
   * component (without scheme) of the URI underlying file
   */
  public JavaPairRDD&lt;HoodieKey, Option&lt;String&gt;&gt; checkExists(JavaRDD&lt;HoodieKey&gt; hoodieKeys) {
<span class="nc" id="L182">    return index.fetchRecordLocation(hoodieKeys, jsc, hoodieTable);</span>
  }

  /**
   * Filter out HoodieRecords that already exists in the output folder. This is useful in deduplication.
   *
   * @param hoodieRecords Input RDD of Hoodie records.
   * @return A subset of hoodieRecords RDD, with existing records filtered out.
   */
  public JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; filterExists(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; hoodieRecords) {
<span class="nc" id="L192">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; recordsWithLocation = tagLocation(hoodieRecords);</span>
<span class="nc bnc" id="L193" title="All 2 branches missed.">    return recordsWithLocation.filter(v1 -&gt; !v1.isCurrentLocationKnown());</span>
  }

  /**
   * Looks up the index and tags each incoming record with a location of a file that contains the row (if it is actually
   * present). Input RDD should contain no duplicates if needed.
   *
   * @param hoodieRecords Input RDD of Hoodie records
   * @return Tagged RDD of Hoodie records
   */
  public JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; tagLocation(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; hoodieRecords) throws HoodieIndexException {
<span class="nc" id="L204">    return index.tagLocation(hoodieRecords, jsc, hoodieTable);</span>
  }

  /**
   * Return all pending compactions with instant time for clients to decide what to compact next.
   * 
   * @return
   */
  public List&lt;Pair&lt;String, HoodieCompactionPlan&gt;&gt; getPendingCompactions() {
<span class="nc" id="L213">    HoodieTableMetaClient metaClient =</span>
<span class="nc" id="L214">        new HoodieTableMetaClient(jsc.hadoopConfiguration(), hoodieTable.getMetaClient().getBasePath(), true);</span>
<span class="nc" id="L215">    return CompactionUtils.getAllPendingCompactionPlans(metaClient).stream()</span>
<span class="nc" id="L216">        .map(</span>
<span class="nc" id="L217">            instantWorkloadPair -&gt; Pair.of(instantWorkloadPair.getKey().getTimestamp(), instantWorkloadPair.getValue()))</span>
<span class="nc" id="L218">        .collect(Collectors.toList());</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>