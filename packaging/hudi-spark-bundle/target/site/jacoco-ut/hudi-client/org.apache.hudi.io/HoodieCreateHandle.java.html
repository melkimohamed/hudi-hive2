<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="fr"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HoodieCreateHandle.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">hudi-spark-bundle_2.11</a> &gt; <a href="../index.html" class="el_bundle">hudi-client</a> &gt; <a href="index.source.html" class="el_package">org.apache.hudi.io</a> &gt; <span class="el_source">HoodieCreateHandle.java</span></div><h1>HoodieCreateHandle.java</h1><pre class="source lang-java linenums">/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * &quot;License&quot;); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hudi.io;

import org.apache.hudi.client.WriteStatus;
import org.apache.hudi.common.model.HoodiePartitionMetadata;
import org.apache.hudi.common.model.HoodieRecord;
import org.apache.hudi.common.model.HoodieRecordLocation;
import org.apache.hudi.common.model.HoodieRecordPayload;
import org.apache.hudi.common.model.HoodieWriteStat;
import org.apache.hudi.common.model.HoodieWriteStat.RuntimeStats;
import org.apache.hudi.common.util.FSUtils;
import org.apache.hudi.common.util.Option;
import org.apache.hudi.config.HoodieWriteConfig;
import org.apache.hudi.exception.HoodieInsertException;
import org.apache.hudi.io.storage.HoodieStorageWriter;
import org.apache.hudi.io.storage.HoodieStorageWriterFactory;
import org.apache.hudi.table.HoodieTable;

import org.apache.avro.generic.GenericRecord;
import org.apache.avro.generic.IndexedRecord;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.spark.TaskContext;

import java.io.IOException;
import java.util.Iterator;

public class HoodieCreateHandle&lt;T extends HoodieRecordPayload&gt; extends HoodieWriteHandle&lt;T&gt; {

<span class="nc" id="L48">  private static final Logger LOG = LogManager.getLogger(HoodieCreateHandle.class);</span>

  private final HoodieStorageWriter&lt;IndexedRecord&gt; storageWriter;
  private final Path path;
<span class="nc" id="L52">  private long recordsWritten = 0;</span>
<span class="nc" id="L53">  private long insertRecordsWritten = 0;</span>
<span class="nc" id="L54">  private long recordsDeleted = 0;</span>
  private Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordIterator;
<span class="nc" id="L56">  private boolean useWriterSchema = false;</span>

  public HoodieCreateHandle(HoodieWriteConfig config, String commitTime, HoodieTable&lt;T&gt; hoodieTable,
      String partitionPath, String fileId) {
<span class="nc" id="L60">    super(config, commitTime, fileId, hoodieTable);</span>
<span class="nc" id="L61">    writeStatus.setFileId(fileId);</span>
<span class="nc" id="L62">    writeStatus.setPartitionPath(partitionPath);</span>

<span class="nc" id="L64">    this.path = makeNewPath(partitionPath);</span>

    try {
<span class="nc" id="L67">      HoodiePartitionMetadata partitionMetadata = new HoodiePartitionMetadata(fs, commitTime,</span>
<span class="nc" id="L68">          new Path(config.getBasePath()), FSUtils.getPartitionPath(config.getBasePath(), partitionPath));</span>
<span class="nc" id="L69">      partitionMetadata.trySave(TaskContext.getPartitionId());</span>
<span class="nc" id="L70">      createMarkerFile(partitionPath);</span>
<span class="nc" id="L71">      this.storageWriter =</span>
<span class="nc" id="L72">          HoodieStorageWriterFactory.getStorageWriter(commitTime, path, hoodieTable, config, writerSchema);</span>
<span class="nc" id="L73">    } catch (IOException e) {</span>
<span class="nc" id="L74">      throw new HoodieInsertException(&quot;Failed to initialize HoodieStorageWriter for path &quot; + path, e);</span>
<span class="nc" id="L75">    }</span>
<span class="nc" id="L76">    LOG.info(&quot;New CreateHandle for partition :&quot; + partitionPath + &quot; with fileId &quot; + fileId);</span>
<span class="nc" id="L77">  }</span>

  /**
   * Called by the compactor code path.
   */
  public HoodieCreateHandle(HoodieWriteConfig config, String commitTime, HoodieTable&lt;T&gt; hoodieTable,
      String partitionPath, String fileId, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordIterator) {
<span class="nc" id="L84">    this(config, commitTime, hoodieTable, partitionPath, fileId);</span>
<span class="nc" id="L85">    this.recordIterator = recordIterator;</span>
<span class="nc" id="L86">    this.useWriterSchema = true;</span>
<span class="nc" id="L87">  }</span>

  @Override
  public boolean canWrite(HoodieRecord record) {
<span class="nc bnc" id="L91" title="All 4 branches missed.">    return storageWriter.canWrite() &amp;&amp; record.getPartitionPath().equals(writeStatus.getPartitionPath());</span>
  }

  /**
   * Perform the actual writing of the given record into the backing file.
   */
  @Override
  public void write(HoodieRecord record, Option&lt;IndexedRecord&gt; avroRecord) {
<span class="nc" id="L99">    Option recordMetadata = record.getData().getMetadata();</span>
    try {
<span class="nc bnc" id="L101" title="All 2 branches missed.">      if (avroRecord.isPresent()) {</span>
        // Convert GenericRecord to GenericRecord with hoodie commit metadata in schema
<span class="nc" id="L103">        IndexedRecord recordWithMetadataInSchema = rewriteRecord((GenericRecord) avroRecord.get());</span>
<span class="nc" id="L104">        storageWriter.writeAvroWithMetadata(recordWithMetadataInSchema, record);</span>
        // update the new location of record, so we know where to find it next
<span class="nc" id="L106">        record.unseal();</span>
<span class="nc" id="L107">        record.setNewLocation(new HoodieRecordLocation(instantTime, writeStatus.getFileId()));</span>
<span class="nc" id="L108">        record.seal();</span>
<span class="nc" id="L109">        recordsWritten++;</span>
<span class="nc" id="L110">        insertRecordsWritten++;</span>
<span class="nc" id="L111">      } else {</span>
<span class="nc" id="L112">        recordsDeleted++;</span>
      }
<span class="nc" id="L114">      writeStatus.markSuccess(record, recordMetadata);</span>
      // deflate record payload after recording success. This will help users access payload as a
      // part of marking
      // record successful.
<span class="nc" id="L118">      record.deflate();</span>
<span class="nc" id="L119">    } catch (Throwable t) {</span>
      // Not throwing exception from here, since we don't want to fail the entire job
      // for a single record
<span class="nc" id="L122">      writeStatus.markFailure(record, t, recordMetadata);</span>
<span class="nc" id="L123">      LOG.error(&quot;Error writing record &quot; + record, t);</span>
<span class="nc" id="L124">    }</span>
<span class="nc" id="L125">  }</span>

  /**
   * Writes all records passed.
   */
  public void write() {
    try {
<span class="nc bnc" id="L132" title="All 2 branches missed.">      while (recordIterator.hasNext()) {</span>
<span class="nc" id="L133">        HoodieRecord&lt;T&gt; record = recordIterator.next();</span>
<span class="nc bnc" id="L134" title="All 2 branches missed.">        if (useWriterSchema) {</span>
<span class="nc" id="L135">          write(record, record.getData().getInsertValue(writerSchema));</span>
        } else {
<span class="nc" id="L137">          write(record, record.getData().getInsertValue(originalSchema));</span>
        }
<span class="nc" id="L139">      }</span>
<span class="nc" id="L140">    } catch (IOException io) {</span>
<span class="nc" id="L141">      throw new HoodieInsertException(&quot;Failed to insert records for path &quot; + path, io);</span>
<span class="nc" id="L142">    }</span>
<span class="nc" id="L143">  }</span>

  @Override
  public WriteStatus getWriteStatus() {
<span class="nc" id="L147">    return writeStatus;</span>
  }

  /**
   * Performs actions to durably, persist the current changes and returns a WriteStatus object.
   */
  @Override
  public WriteStatus close() {
<span class="nc" id="L155">    LOG</span>
<span class="nc" id="L156">        .info(&quot;Closing the file &quot; + writeStatus.getFileId() + &quot; as we are done with all the records &quot; + recordsWritten);</span>
    try {

<span class="nc" id="L159">      storageWriter.close();</span>

<span class="nc" id="L161">      HoodieWriteStat stat = new HoodieWriteStat();</span>
<span class="nc" id="L162">      stat.setPartitionPath(writeStatus.getPartitionPath());</span>
<span class="nc" id="L163">      stat.setNumWrites(recordsWritten);</span>
<span class="nc" id="L164">      stat.setNumDeletes(recordsDeleted);</span>
<span class="nc" id="L165">      stat.setNumInserts(insertRecordsWritten);</span>
<span class="nc" id="L166">      stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);</span>
<span class="nc" id="L167">      stat.setFileId(writeStatus.getFileId());</span>
<span class="nc" id="L168">      stat.setPath(new Path(config.getBasePath()), path);</span>
<span class="nc" id="L169">      long fileSizeInBytes = FSUtils.getFileSize(fs, path);</span>
<span class="nc" id="L170">      stat.setTotalWriteBytes(fileSizeInBytes);</span>
<span class="nc" id="L171">      stat.setFileSizeInBytes(fileSizeInBytes);</span>
<span class="nc" id="L172">      stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());</span>
<span class="nc" id="L173">      RuntimeStats runtimeStats = new RuntimeStats();</span>
<span class="nc" id="L174">      runtimeStats.setTotalCreateTime(timer.endTimer());</span>
<span class="nc" id="L175">      stat.setRuntimeStats(runtimeStats);</span>
<span class="nc" id="L176">      writeStatus.setStat(stat);</span>

<span class="nc" id="L178">      LOG.info(String.format(&quot;CreateHandle for partitionPath %s fileID %s, took %d ms.&quot;, stat.getPartitionPath(),</span>
<span class="nc" id="L179">          stat.getFileId(), runtimeStats.getTotalCreateTime()));</span>

<span class="nc" id="L181">      return writeStatus;</span>
<span class="nc" id="L182">    } catch (IOException e) {</span>
<span class="nc" id="L183">      throw new HoodieInsertException(&quot;Failed to close the Insert Handle for path &quot; + path, e);</span>
    }
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.5.201910111838</span></div></body></html>